{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "University of Artificial Intelligence. Glossary extraction. Outside Test.2020. v.0.12.12.11.min",
      "provenance": [],
      "collapsed_sections": [
        "yc9STq14ecp8",
        "GZyCt7bwQvMt",
        "kvLTZMldti8v",
        "p5qlEXQxY0mA",
        "SpOYDpNQZhvt",
        "exHVu42xsbWr",
        "2hWix19rBuc4",
        "q-H265oSr-RE",
        "7zXv_Ad7dsZJ",
        "4qQTEy0vh8CQ",
        "eg42wHw3aBXL",
        "zvDohHpAkost",
        "ZsIIMxW5crfv",
        "b-8OLgCqNPHW",
        "1UlIqDuCsHN5",
        "-FC9VX4zsnJa",
        "MWY12oi3c65O",
        "NOdTHaTVwmqZ",
        "9wRfEL6FxhVk"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubygitflow/Glossary-extraction/blob/main/University_of_Artificial_Intelligence_Glossary_extraction_Outside_Test_2020_v_0_12_12_11_min.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3qOPD_U0ag_"
      },
      "source": [
        "# Glossary extraction. \n",
        "\n",
        "### The part-of-speech way. \n",
        "### Multiple Inputs: Lexemes, parts of speech, morphological features, alphabet.\n",
        "\n",
        "https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/ \n",
        "\n",
        "**The research project**\n",
        "\n",
        "В текущей версии: \n",
        "* реализован минималистичный вариант для обучения нейронной сети без тщательных проверок и для её тестирования на текстах из внешних файловых источников."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc9STq14ecp8"
      },
      "source": [
        "# **Service data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oNlKeCH0tYI"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGmMSJDHWfug"
      },
      "source": [
        "Ссылки на файлы данных:\n",
        "\n",
        "База_словарей_train_6.docx : https://drive.google.com/file/d/13QKXApDf8q8R_BYviX1J80BCJXUOlZUw/view?usp=sharing\n",
        "\n",
        "База_словарей_test_6.docx : https://drive.google.com/file/d/19a1rOu6NuAb-Y_HrHhN7Ssg_ZvxTXHhH/view?usp=sharing\n",
        "\n",
        "База_словарей_train_6.xml : https://drive.google.com/file/d/16lNQFww1QyYRxr8S4OsA2THypqxLQOra/view?usp=sharing\n",
        "\n",
        "База_словарей_test_6.xml : https://drive.google.com/file/d/1-9fG7sdiZnjkarvitPYgX9PFt95eErPi/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CYCV7DDe-5l"
      },
      "source": [
        "color_meaning = {'00ffff':'термин', 'ffff00':'связка', '00ff00':'определение', 'd3d3d3':'расшифровка', \n",
        "                 'ff00ff':'дубль-термин', 'ff0000':'дубль-связка', '808000':'дубль-определение', '0000ff':'дубль-расшифровка',\n",
        "                 'cyan':'термин','yellow':'связка','green':'определение', 'lightgray':'расшифровка',\t                 \n",
        "                 'magenta':'дубль-термин', 'red':'дубль-связка','darkyellow':'дубль-определение', 'blue':'дубль-расшифровка'}\n",
        "\n",
        "                 #создаем словарь соответствий цвет-значение   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWtUZO-tpKl6"
      },
      "source": [
        "color_dic = {'термин':0,'связка':1,'определение':2,'расшифровка':3,\n",
        "\t\t\t       'дубль-термин':4,'дубль-связка':5,'дубль-определение':6,'дубль-расшифровка':7}\n",
        "dic_color = {value:key for key, value in color_dic.items()}\n",
        "\n",
        "zero_symbol = 'o'\n",
        "\n",
        "colors_count = len(color_dic) + 1 # количество уникальных тегов (не забываем про тег отсутствия класса для разметки)\n",
        "\n",
        "skip_amount = colors_count  # кол-во пропускаемых строк под легенду при считывании данных (здесь учитывается, что есть пусая строка, в списке тегов последнее значение используется под отсутствующий класс для разметки)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNXWVxcsGg2Z"
      },
      "source": [
        "colors_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1YJ-aoYe-5q"
      },
      "source": [
        "# названия необходимых тегов для извлекаемых категорий\n",
        "r = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}r'\n",
        "rpr = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}rPr'\n",
        "t = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}t'\n",
        "shd = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}shd'\n",
        "fill = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}fill'\n",
        "highlight = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}highlight'\n",
        "val = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALyGh7kCS7NV"
      },
      "source": [
        "''' \n",
        "  Словарь для цветовой разметки разобранного текста\n",
        "'''\n",
        "class color:\n",
        "  GREY    = '\\33[90m'\n",
        "  RED2    = '\\33[91m'\n",
        "  GREEN2  = '\\33[92m'\n",
        "  YELLOW2 = '\\33[93m'\n",
        "  BLUE2   = '\\33[94m'\n",
        "  VIOLET2 = '\\33[95m'\n",
        "  LI_CYAN = '\\33[96m'\n",
        "  WHITE2  = '\\33[97m'\n",
        "\n",
        "  BLACK  = '\\33[30m'\n",
        "  RED    = '\\33[31m'\n",
        "  GREEN  = '\\33[32m'\n",
        "  YELLOW = '\\33[33m'\n",
        "  BLUE   = '\\33[34m'\n",
        "  VIOLET = '\\33[35m'\n",
        "  CYAN   = '\\33[36m'\n",
        "  WHITE  = '\\33[37m'\n",
        "\n",
        "  GREYBG    = '\\33[100m'\n",
        "  REDBG2    = '\\33[101m'\n",
        "  GREENBG2  = '\\33[102m'\n",
        "  YELLOWBG2 = '\\33[103m'\n",
        "  BLUEBG2   = '\\33[104m'\n",
        "  VIOLETBG2 = '\\33[105m'\n",
        "  LI_CYANBG = '\\33[106m'\n",
        "  WHITEBG2  = '\\33[107m'\n",
        "\n",
        "  BLACKBG  = '\\33[40m'\n",
        "  REDBG    = '\\33[41m'\n",
        "  GREENBG  = '\\33[42m'\n",
        "  YELLOWBG = '\\33[43m'\n",
        "  BLUEBG   = '\\33[44m'\n",
        "  VIOLETBG = '\\33[45m'\n",
        "  CYANBG   = '\\33[46m'\n",
        "  WHITEBG  = '\\33[47m'\n",
        "\n",
        "\n",
        "  BOLD \t\t  = '\\33[1m'\n",
        "  ITALIC \t  = '\\33[3m'\n",
        "  UNDERLINE = '\\33[4m'\n",
        "\n",
        "  BLINK    = '\\33[5m'\n",
        "  BLINK2   = '\\33[6m'\n",
        "  SELECTED = '\\33[7m'\n",
        "  END \t\t = '\\33[0m'\n",
        "  \n",
        "dic_colored_items = {0:color.LI_CYANBG+color.BLACK, 1:color.YELLOWBG2+color.BLACK, 2:color.GREENBG2+color.BLACK, 3:color.WHITEBG+color.BLACK,\n",
        "\t\t\t               4:color.VIOLETBG2+color.WHITE2, 5:color.REDBG2+color.WHITE2, 6:color.YELLOWBG+color.BLACK, 7:color.BLUEBG+color.WHITE2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NguG3E7fNQDC"
      },
      "source": [
        "## Connecting libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxSyLcXUjCB5"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k2x5MTeMPjK"
      },
      "source": [
        "!pip install polyglot\n",
        "# https://github.com/aboSamoor/polyglot/issues/152\n",
        "!pip install PyICU\n",
        "!pip install pycld2\n",
        "!pip install morfessor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hyWYiPqLdez"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model # Импортируем модели keras\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, concatenate, Activation, MaxPooling1D, Conv1D, concatenate  # Импортируем стандартные слои keras\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, Conv2DTranspose, Lambda, LSTM, GRU, Bidirectional, GlobalMaxPool1D # Импортируем стандартные слои keras\n",
        "from tensorflow.keras.layers import SpatialDropout1D \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras import backend as K # Импортируем модуль backend keras'а\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, Adadelta\n",
        "from tensorflow.keras import utils # Импортируем модуль utils библиотеки tensorflow.keras для получения OHE-представления\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt \n",
        "import collections\n",
        "%matplotlib inline\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report # статистика точности предсказания по классам\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import re # https://habr.com/ru/post/349860/\n",
        "import pymorphy2 # https://pymorphy2.readthedocs.io/en/latest/user/guide.html\n",
        "import polyglot # https://polyglot.readthedocs.io/en/latest/\n",
        "from polyglot.detect import Detector # https://polyglot.readthedocs.io/en/latest/Detection.html\n",
        "from textblob import TextBlob # https://textblob.readthedocs.io/en/latest/quickstart.html\n",
        "# import nltk\n",
        "# from nltk import pos_tag # https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "import os # Импортируем библиотеку для работы с файловой системой\n",
        "from google.colab import files # Импортируем Модуль для работы с файлами\n",
        "from google.colab import drive # Подключаем гугл-диск\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UZV7UcJ6qyw"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP4-NkAt96gv"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB1gdZbhsmLA"
      },
      "source": [
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk_b6i6_Ho5e"
      },
      "source": [
        "all_tags_together = ['o термин связка определение расшифровка дубль-термин дубль-связка дубль-определение дубль-расшифровка']\r\n",
        "tag_tokenizer = Tokenizer(filters=' ')\r\n",
        "tag_tokenizer.fit_on_texts(all_tags_together)\r\n",
        "tag_tokenizer.index_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFr4oROCKEBQ"
      },
      "source": [
        "# prepare color dictionary\r\n",
        "tag_tokenizer_dic_color = {}\r\n",
        "tag_tokenizer_dic_color[1] = color.END\r\n",
        "for i in range(2,10):\r\n",
        "  tag_tokenizer_dic_color[i] = dic_colored_items[color_dic[tag_tokenizer.index_word[i]]]\r\n",
        "tag_tokenizer_dic_color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZyCt7bwQvMt"
      },
      "source": [
        "## Library of local Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9TmopkIBZyF"
      },
      "source": [
        "'''\n",
        "  Сервиcная функция:\n",
        "  Лемматизация для английского языка\n",
        "  Функция предполагает разбор именно слов (без пробелов внутри текста - Это важно), \n",
        "  но лучшим оказался алгоритм лемматизации предложений\n",
        "'''\n",
        "# https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/\n",
        "def textblob_lemmatize(blob):\n",
        "  sent = TextBlob(blob)\n",
        "  tag_dict = {\"J\": 'a', \n",
        "              \"N\": 'n', \n",
        "              \"V\": 'v', \n",
        "              \"R\": 'r'}\n",
        "  words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
        "  lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
        "  res = \"\".join(lemmatized_list)\n",
        "  return res if res!='' else blob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR58OhlWZnfC"
      },
      "source": [
        "'''\n",
        "  Сервиcная функция:\n",
        "  Извлечение тега для лексемы на английском\n",
        "  https://www.geeksforgeeks.org/python-part-of-speech-tagging-using-textblob/\n",
        "  CC coordinating conjunction\n",
        "  CD cardinal digit\n",
        "  DT determiner\n",
        "  EX existential there (like: “there is” … think of it like “there exists”)\n",
        "  FW foreign word\n",
        "  IN preposition/subordinating conjunction\n",
        "  JJ adjective ‘big’\n",
        "  JJR adjective, comparative ‘bigger’\n",
        "  JJS adjective, superlative ‘biggest’\n",
        "  LS list marker 1)\n",
        "  MD modal could, will\n",
        "  NN noun, singular ‘desk’\n",
        "  NNS noun plural ‘desks’\n",
        "  NNP proper noun, singular ‘Harrison’\n",
        "  NNPS proper noun, plural ‘Americans’\n",
        "  PDT predeterminer ‘all the kids’\n",
        "  POS possessive ending parent‘s\n",
        "  PRP personal pronoun I, he, she\n",
        "  PRP$ possessive pronoun my, his, hers\n",
        "  RB adverb very, silently,\n",
        "  RBR adverb, comparative better\n",
        "  RBS adverb, superlative best\n",
        "  RP particle give up\n",
        "  TO to go ‘to‘ the store.\n",
        "  UH interjection errrrrrrrm\n",
        "  VB verb, base form take\n",
        "  VBD verb, past tense took\n",
        "  VBG verb, gerund/present participle taking\n",
        "  VBN verb, past participle taken\n",
        "  VBP verb, sing. present, non-3d take\n",
        "  VBZ verb, 3rd person sing. present takes\n",
        "  WDT wh-determiner which\n",
        "  WP wh-pronoun who, what\n",
        "  WP$ possessive wh-pronoun whose\n",
        "  WRB wh-abverb where, when\n",
        "\n",
        "  https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html#russian-numbers\n",
        "  Граммема\tЗначение\t          Примеры\n",
        "  sing\t    единственное число\tхомяк, говорит\n",
        "  plur\t    множественное число\tхомяки, говорят\n",
        "'''\n",
        "def textblob_tags(blob):\n",
        "  # print('textblob_tags:',blob, TextBlob(blob).tags)\n",
        "  try:\n",
        "    pos = TextBlob(blob).tags[0][1]\n",
        "    if pos in ['NN', 'NNP']:\n",
        "      numbers = 'sing'\n",
        "    elif pos in ['NNS', 'NNPS']:\n",
        "      numbers = 'plur'\n",
        "    else:\n",
        "      numbers = 'None'\n",
        "  except:\n",
        "    pos = 'NN'\n",
        "    numbers = 'sing'\n",
        "\n",
        "  return pos, numbers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAnHLGGYaAEe"
      },
      "source": [
        "'''\n",
        "  Сервиcная функция:\n",
        "  Назначение граммемы\n",
        "  https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html\n",
        "'''\n",
        "def define_morphy( \n",
        "  pos = 'None', animacy = 'None', aspect = 'None',\n",
        "  case = 'None', gender = 'None', involvement = 'None',\n",
        "  mood = 'None', number = 'None', person = 'None',\n",
        "  tense = 'None', transitivity = 'None', voice = 'None'\n",
        "): \n",
        "  return [str(pos), str(animacy), str(aspect), str(case), str(gender), str(involvement), \n",
        "          str(mood), str(number), str(person), str(tense), str(transitivity), str(voice)]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6qbZAv0GESe"
      },
      "source": [
        "'''\n",
        "  Сервиcная функция:\n",
        "  Извлечение морфологических признаков лексемы \n",
        "'''\n",
        "def morphyus(blob):\n",
        "  # 1. язык: долюно быть - sign, en, ru, el. Другие пока не расматриваются\n",
        "  # 2. С заглавной ли буквы написание слова\n",
        "  # итерации по\n",
        "    # 3. слово в нормальной форме\n",
        "    # 4. часть речи\n",
        "    # 5. всё остальное\n",
        "\n",
        "  draw_in = lambda a: ''.join(a.split())\n",
        "\n",
        "  \n",
        "  morphy = []  \n",
        "  tags = morph.parse(blob)\n",
        "  # print('tags',tags)\n",
        "  try:\n",
        "    detector = Detector(blob)\n",
        "    language = detector.language.code\n",
        "    # print('language',language)\n",
        "  except:\n",
        "    lng_lst = str(tags[0].tag).split(',')\n",
        "    # print('lng_lst',lng_lst)\n",
        "    if lng_lst[0]=='UNKN':\n",
        "      morphy.append('sign')\n",
        "      morphy.append('true' if blob[0].isupper() else 'false')\n",
        "      morphy.append(textblob_lemmatize(blob).lower())\n",
        "      pos, number = textblob_tags(blob)\n",
        "      morphy += define_morphy(pos = pos, number = number)         \n",
        "      return morphy\n",
        "    elif lng_lst[0]=='LATN':\n",
        "      morphy.append('en')\n",
        "      morphy.append('true' if blob[0].isupper() else 'false')\n",
        "      morphy.append(textblob_lemmatize(blob).lower())\n",
        "      pos, number = textblob_tags(blob)\n",
        "      morphy += define_morphy(pos = pos, number = number)  \n",
        "      return morphy\n",
        "    elif lng_lst[0]=='NUMB':\n",
        "      morphy.append('sign')\n",
        "      morphy.append('false')\n",
        "      morphy.append(draw_in(tags[0].normal_form)) # allways .lower()\n",
        "      morphy += define_morphy(pos = 'NUMB', number = lng_lst[1])  \n",
        "      return morphy\n",
        "    elif lng_lst[0]=='ROMN':\n",
        "      morphy.append('en')\n",
        "      morphy.append('true' if blob[0].isupper() else 'false')\n",
        "      morphy.append(textblob_lemmatize(blob).lower())\n",
        "      pos, number = textblob_tags(blob)\n",
        "      morphy += define_morphy(pos = pos, number = number)  \n",
        "      # print 'ROMN' morfology\n",
        "      morphy.append(draw_in(tags[0].normal_form)) # allways .lower()\n",
        "      morphy += define_morphy(pos = 'ROMN')\n",
        "      return morphy\n",
        "    elif lng_lst[0]=='PNCT':\n",
        "      morphy.append('sign')\n",
        "      morphy.append('false')\n",
        "      morphy.append(draw_in(tags[0].normal_form)) # allways .lower()\n",
        "      morphy += define_morphy(pos = 'PNCT')  \n",
        "      return morphy\n",
        "    elif lng_lst[0] in ['NOUN','ADJF','ADJS','COMP','VERB','INFN','PRTF','PRTS','GRND','NUMR','ADVB','NPRO','PRED','PREP','CONJ','PRCL','INTJ']:\n",
        "      language = 'ru'\n",
        "    else:\n",
        "      language = lng_lst[0]\n",
        "\n",
        "  # print('language:',language, '; blob:', blob)\n",
        "  if language in ['bg', 'uk', 'mn', 'ab', 'ba', 'be', 'ky', 'mk', 'ro', 'ru', 'sr', 'tk', 'tt', 'ug', 'uz', 'ug']: \n",
        "  # на самом деле плохое условие. Надо менять алгоритм работы с детектором языка - detector.language\n",
        "    language = 'ru'\n",
        "    morphy.append('ru')\n",
        "    morphy.append('true' if blob[0].isupper() else 'false')\n",
        "    for elem in tags:\n",
        "      morphy.append(elem.normal_form) # allways .lower()\n",
        "      morphy += define_morphy(pos = elem.tag.POS,\n",
        "                              animacy = elem.tag.animacy, \n",
        "                              aspect = elem.tag.aspect,\n",
        "                              case = elem.tag.case, \n",
        "                              gender = elem.tag.gender, \n",
        "                              involvement = elem.tag.involvement,\n",
        "                              mood = elem.tag.mood, \n",
        "                              number = elem.tag.number,\n",
        "                              person = elem.tag.person,\n",
        "                              tense = elem.tag.tense,\n",
        "                              transitivity = elem.tag.transitivity,\n",
        "                              voice = elem.tag.voice)\n",
        "  else:\n",
        "    # en - ak da de gl hu la mt nl sco sl sm \n",
        "    if language != 'el':\n",
        "      language = 'en'\n",
        "    morphy.append(language)\n",
        "    morphy.append('true' if blob[0].isupper() else 'false')\n",
        "    morphy.append(textblob_lemmatize(blob).lower())\n",
        "    pos, number = textblob_tags(blob)\n",
        "    morphy += define_morphy(pos = pos, number = number)  \n",
        "    if str(tags[0].tag)=='ROMN':\n",
        "      morphy.append(tags[0].normal_form)\n",
        "      morphy += define_morphy(pos = 'ROMN')\n",
        "\n",
        "  return morphy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8vghf1KdWvX"
      },
      "source": [
        "'''\n",
        "  Сервиcная функция:\n",
        "  Загрузка выборки заданного количества фраз из файла \n",
        "'''\n",
        "def load_xml(filename, paragraphs_amount):\n",
        "    test_tree = ET.parse(filename)\n",
        "\n",
        "    test_root = test_tree.getroot()\n",
        "    test_body = test_root[0]\n",
        "    test_row_count = paragraphs_amount + skip_amount\n",
        "    return list(iter(test_body))[:test_row_count]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P76fRbVdCZH"
      },
      "source": [
        "'''\n",
        "  Сервиcная функция:\n",
        "  Вставка пробела в заданной позиции в строке\n",
        "'''\n",
        "def insert_space(text, index):\n",
        "  return text[:index] + ' ' + text[index:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te8tUbzqXZOv"
      },
      "source": [
        "'''\n",
        "  Основная функция для подготовки обучающей выборки.\n",
        "  Создаём параллельные массивы с текстом — paragraphs,\n",
        "  категориями лексем — tags\n",
        "  и классификацией предложений — semantics\n",
        "'''\n",
        "def pars_xml(ps):\n",
        "    # Массивы данных по параграфам\n",
        "    paragraphs = []\n",
        "    tags = []\n",
        "    semantics = []\n",
        "    exception_amount = 0\n",
        "    exception_array = []\n",
        "    # Массив фактических цветов в файле исходных данных\n",
        "    colors = set() # все цвета в документе\n",
        "    # длина массива лексем в параграфе\n",
        "    max_paragraph_len = 0  # Максимальная длина предложения\n",
        "    max_paragraph_len_ind = -1  # Индекс предложения с максимальной длиной\n",
        "\n",
        "    for i, paragraph in enumerate(ps): # проходим по всем абзацам\n",
        "        z_item = [0] # массив семантических индикаторов -  наличие определительной струткуры первого и второго уровня\n",
        "        if i > colors_count - 1:     # пропускаем легенду для цветовой разметки    \n",
        "            phrases_list = paragraph.findall(r) # находим все фразы (часть новости)\n",
        "            p_words = [] # список сущностей в абзаце\n",
        "            tags_w = [] # список семантических цветов для слов в абзаце\n",
        "\n",
        "            last_ind = len(phrases_list)-1\n",
        "            for ind, phrase in enumerate(phrases_list): # проходим по всем фразам в абзаце\n",
        "                words = [] # список сущностей во фразе\n",
        "                y_list = [] # категории для сущностей во фразе\n",
        "\n",
        "                try:\n",
        "                  text = phrase.find(t).text # получаем текст, содержащийся во фразе\n",
        "\n",
        "                  # 1) выделяем пробелом точку (воскл.зн., вопр.зн., двоеточние) в конце предложения\n",
        "                  text_comma = text.strip()\n",
        "                  len_text = len(text_comma)-1\n",
        "                  if ind==last_ind and len_text>0: \n",
        "                    if text_comma[len_text] in ['.', ':', '!', '?']:\n",
        "                      text_comma = insert_space(text_comma, len_text)\n",
        "                    \n",
        "                  # 2) выделяем пробелами символы: , { } ( ) [ ] ; « » „ “ \"\n",
        "                  text_comma = text_comma.replace(',', ' , ')\n",
        "                  text_comma = text_comma.replace('(', ' ( ')\n",
        "                  text_comma = text_comma.replace(')', ' ) ')\n",
        "                  text_comma = text_comma.replace('[', ' [ ')\n",
        "                  text_comma = text_comma.replace(']', ' ] ')\n",
        "                  text_comma = text_comma.replace('{', ' { ')\n",
        "                  text_comma = text_comma.replace('}', ' } ')\n",
        "                  text_comma = text_comma.replace('\"', ' \" ')\n",
        "                  text_comma = text_comma.replace('„', ' „ ')\n",
        "                  text_comma = text_comma.replace('“', ' “ ')\n",
        "                  text_comma = text_comma.replace('«', ' « ')\n",
        "                  text_comma = text_comma.replace('»', ' » ')\n",
        "                  text_comma = text_comma.replace(';', ' ; ')\n",
        "                  text_comma = text_comma.replace('%', ' %') # вставляем пробел именно слева, но не справа\n",
        "                  text_comma = text_comma.replace('—', ' — ')\n",
        "                  \n",
        "            \n",
        "                  # 3) заменяем все виды пробелов одиночным пробелом #32  \n",
        "                  text_comma = text_comma.replace('\\xa0', '')\n",
        "                  text_comma = text_comma.replace('\\x301', '')\n",
        "                  text_comma = text_comma.replace('\\u0301', '')\n",
        "                  text_comma = text_comma.replace('\\ufeff', '')\n",
        "                  text_comma = text_comma.replace('  ', ' ')\n",
        "                  text_comma = text_comma.replace('  ', ' ')\n",
        "                  text_comma = text_comma.strip()      \n",
        "\n",
        "                  # 4) выделяем левым пробелом двоеточие (:) при одновременном выполнении условий: \n",
        "                  # после двух буквенных символов в любом алфавите и перед правым пробелом. \n",
        "                  # https://habr.com/ru/post/349860/\n",
        "                  text_comma = re.sub(r'(\\w\\w)(:)(\\s)',  r'\\1 \\2\\3', text_comma)\n",
        "                  text_comma = re.sub(r'(\\w\\w)(:)$',  r'\\1 \\2', text_comma)\n",
        "\n",
        "                  # отделяем пробелом слова в кириллице внутри угловых скобок — <ммм> \n",
        "                  text_comma = re.sub(r'(а..я)(>)',  r'\\1 \\2', text_comma)\n",
        "                  text_comma = re.sub(r'(<)(а..я)',  r'\\1 \\2', text_comma)\n",
        "\n",
        "\n",
        "                  if (len(text_comma)>0):\n",
        "                    style = phrase.find(rpr) # получаем стили фразы\n",
        "                    \n",
        "                    if style.find(shd) is not None: # если размечали через заливку\n",
        "                        color = style.find(shd).attrib[fill] # получаем значение цвета заливки\n",
        "                    elif style.find(highlight) is not None: # если размечали через хайлайт\n",
        "                        color = style.find(highlight).attrib[val] # получаем значение цвета хайлайта\n",
        "                    else:\n",
        "                        color = 'white' # иных вариантов выделения в word нет, значит эта фраза не выделена (белый цвет)\n",
        "                    color = color.lower() # переводим строковое значение цвета в нижний регистр\n",
        "                    meaning = color_meaning[color] if color in color_meaning else '' # если есть цвет в словаре цвет-значение, то получаем значение. в противном случае у фразы значение не было выделено\n",
        "                    \n",
        "                    colors.add(color) # добавляем цвет в словарь всех встреченных цветов, если нужно проанализировать их\n",
        "                    \n",
        "                    words = text_comma.split()\n",
        "                    k = len(words)\n",
        "                    if meaning in color_dic: # если во фразе присутствует какая-то выделяемая сущность\n",
        "                      z_item = [1] # отмечаем фразу с понятийной структурой\n",
        "                      for j in range(k):\n",
        "                        y_list.append(meaning) # получаем индекс позиции, соответствующей какой-то семантической(смысловой) окраске и устанавливаем по этому индексу абсолютное значение категории\n",
        "                    else:\n",
        "                      for j in range(k):\n",
        "                        y_list.append(zero_symbol)\n",
        "            \n",
        "                    p_words += words  # список сущностей в параграфе\n",
        "                    tags_w += y_list # список семантических цветов для слов в параграфе\n",
        "\n",
        "                except Exception:\n",
        "                  exception_amount += 1\n",
        "                  exception_array.append(i)\n",
        "\n",
        "            # print(i, p_words)\n",
        "            if p_words and len(p_words)>3: # количество слов в предложении позволяет иметь в себе определительную структуру \n",
        "                                           # как минимум из 3-х слов и завершающего знака препинания\n",
        "              paragraphs.append(p_words)\n",
        "              tags.append(tags_w)\n",
        "              semantics.append(z_item) # записываем семантику\n",
        "              \n",
        "            if len(p_words) >= max_paragraph_len: \n",
        "              max_paragraph_len = len(p_words)\n",
        "              max_paragraph_len_ind = i - skip_amount\n",
        "\n",
        "        if i%100==0:\n",
        "            print('line',i)\n",
        "\n",
        "    return     paragraphs, tags, colors, max_paragraph_len, max_paragraph_len_ind, exception_array, semantics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hie9K_i1Qy3h"
      },
      "source": [
        "'''\n",
        "  Графическая функция:\n",
        "  Подсчёт и вывод на печать статистики о содержании категорий определительных структур в размеченных текстах\n",
        "'''\n",
        "def class_stat(paragraphs, tags, ext_print = False):\n",
        "  classes = np.zeros((colors_count))\n",
        "  len_tags = len(tags)\n",
        "  print('Длина размеченной выборки:', len_tags)\n",
        "  if ext_print:\n",
        "    print(\"\\nПеречень фраз со 100% разметкой:\")\n",
        "  for i, elem in enumerate(tags):\n",
        "    if zero_symbol in elem:\n",
        "      if len(set(elem))==1:\n",
        "        classes[0] += 1\n",
        "    else:\n",
        "      if ext_print:\n",
        "        print(\"paragraphs[{}]\".format(i), '=',paragraphs[i])\n",
        "        print(\"tags[{}]\".format(i), '=',tags[i])\n",
        "        print(\" \")\n",
        "    for item in list(color_dic.keys()):\n",
        "      if item in elem:\n",
        "        real_index = int(color_dic[item]) + 1\n",
        "        classes[real_index] += 1\n",
        "  print(\"\\nСтатистика по классам\")\n",
        "  for i in range(len(classes)):\n",
        "    if i==0:\n",
        "      print(f'Фраз без определительных структур: {classes[0]} - {round(classes[0]*100./len_tags,2)}%')\n",
        "    else:\n",
        "      key=list(color_dic.keys())[list(color_dic.values()).index(i-1)] # ключ по значению\n",
        "      print(f'Фраз с категорией {key}: {classes[i]} - {round(classes[i]*100./len_tags,2)}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuOa4BIgoyRM"
      },
      "source": [
        "'''\n",
        "  Графическая функция:\n",
        "  Построение статистических графиков по входным данным.\n",
        "'''\n",
        "def draw_paragraphs_stat(paragraphs):\n",
        "  # https://pyprog.pro/mpl/mpl_bar.html\n",
        "  counter = []\n",
        "  for elem in paragraphs:\n",
        "    counter.append(len(elem))\n",
        "\n",
        "  plt.figure(figsize=(27, 5))\n",
        "  plt.grid(True)\n",
        "  plt.bar(range(len(paragraphs)), counter)\n",
        "  plt.suptitle('Фактическое распределение длин абзацев (в лексемах)')\n",
        "  plt.show()\n",
        "\n",
        "  dic_counter = collections.Counter(counter)\n",
        "  print('Collections',dic_counter)\n",
        "\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  plt.grid(True)\n",
        "  plt.bar(dic_counter.keys(), dic_counter.values())\n",
        "  plt.suptitle('Статистика кол-ва абзацев по их длинам')\n",
        "  plt.show()\n",
        "\n",
        "  return counter, dic_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbHUueD3Q7uU"
      },
      "source": [
        "'''\n",
        "  Сервисная функция:\n",
        "  Сделать разметку текста цветом\n",
        "'''\n",
        "def tokens_to_text(x_arr, y_arr, max_len = 0):\n",
        "  res = '' \n",
        "  str_len = 0\n",
        "  def add_col(col, txt):\n",
        "    nonlocal res, str_len\n",
        "    if max_len > 0:\n",
        "      next_len = len(txt)\n",
        "      if str_len + next_len > max_len:\n",
        "        res = res + col + '\\n ' + txt\n",
        "        str_len = next_len + 1\n",
        "      else:\n",
        "        res = res + col + ' ' + txt\n",
        "        str_len += next_len + 1\n",
        "    else:\n",
        "      res = res + col + ' ' + txt\n",
        "\n",
        "  for i, el in enumerate(x_arr):\n",
        "    if el==0:\n",
        "      break\n",
        "    col_ind = y_arr[i].argmax() + 1\n",
        "    add_col(tag_tokenizer_dic_color[col_ind], tokenizer.index_word[el])\n",
        "\n",
        "  return res + color.END"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg58h8P1_xkc"
      },
      "source": [
        "'''\n",
        "  Сервисная функция:\n",
        "  Сделать цветовую разметку текста\n",
        "'''\n",
        "def colorize_the_text(x_arr, y_arr, max_len = 0, padding = 'post', padding_len = 128):\n",
        "  res = '' \n",
        "  str_len = 0\n",
        "  def add_col(col, txt):\n",
        "    nonlocal res, str_len\n",
        "    if max_len > 0:\n",
        "      next_len = len(txt)\n",
        "      if str_len + next_len > max_len:\n",
        "        res = res + col + '\\n ' + txt\n",
        "        str_len = next_len + 1\n",
        "      else:\n",
        "        res = res + col + ' ' + txt\n",
        "        str_len += next_len + 1\n",
        "    else:\n",
        "      res = res + col + ' ' + txt\n",
        "\n",
        "  start_y_arr = padding_len - len(x_arr)\n",
        "  for i, el in enumerate(x_arr):\n",
        "    if padding == 'post':\n",
        "      col_ind = y_arr[i].argmax() + 1\n",
        "    else:\n",
        "      j = start_y_arr + i\n",
        "      col_ind = y_arr[j].argmax() + 1\n",
        "    add_col(tag_tokenizer_dic_color[col_ind], el)\n",
        "\n",
        "  return res + color.END"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snnYNd4K2dhG"
      },
      "source": [
        "'''\n",
        "  Сервисная функция для обучающей модели:\n",
        "  Удалить неполные данные из массива с разметкой, полученной методом сегментации:\n",
        "  — фразы без разметки;\n",
        "  — один выделенный класс во фразе;\n",
        "  — две лексемы на два выделенных класса.\n",
        "'''\n",
        "def cleansed_data(Ypred, Ytest, paragraphs):\n",
        "  Ypred_res = []\n",
        "  Ytest_res = []\n",
        "  paragraphs_res = []\n",
        "  delete = 0\n",
        "  for i,phrase in enumerate(Ypred):\n",
        "    uniq = set()\n",
        "    all = []\n",
        "    for elem in phrase:\n",
        "      ind = np.argmax(elem)\n",
        "      if ind>0:\n",
        "        uniq.add(ind)\n",
        "        all.append(ind)\n",
        "    if len(uniq)>1 and len(all)>2:\n",
        "      Ypred_res.append(phrase)\n",
        "      Ytest_res.append(Ytest[i])\n",
        "      paragraphs_res.append(paragraphs[i])\n",
        "    else:\n",
        "      delete += 1\n",
        "  if delete>0:\n",
        "    print(f'Очищено фраз: {delete}')\n",
        "  return np.array(Ypred_res), np.array(Ytest_res), np.array(paragraphs_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN54otIJ713l"
      },
      "source": [
        "'''\r\n",
        "  Сервисная функция для рабочей модели:\r\n",
        "  Удалить неполные данные из массива с разметкой, полученной методом сегментации:\r\n",
        "  — фразы без разметки;\r\n",
        "  — один выделенный класс во фразе;\r\n",
        "  — две лексемы на два выделенных класса;\r\n",
        "  — фразы без \"Термина\" или \"Дубль-Термина\".\r\n",
        "'''\r\n",
        "def cleansed_work_data(Ypred, paragraphs):\r\n",
        "  Ypred_res = []\r\n",
        "  paragraphs_res = []\r\n",
        "  delete = 0\r\n",
        "  for i,phrase in enumerate(Ypred):\r\n",
        "    uniq = set()\r\n",
        "    all = []\r\n",
        "    is_term = False\r\n",
        "    for elem in phrase:\r\n",
        "      ind = np.argmax(elem)\r\n",
        "      if ind>0:\r\n",
        "        uniq.add(ind)\r\n",
        "        all.append(ind)\r\n",
        "        if tag_tokenizer.index_word[ind + 1] in ['термин','дубль-термин']:\r\n",
        "          is_term = True\r\n",
        "    if len(uniq)>1 and len(all)>2 and is_term:\r\n",
        "      Ypred_res.append(phrase)\r\n",
        "      paragraphs_res.append(paragraphs[i])\r\n",
        "    else:\r\n",
        "      delete += 1\r\n",
        "  if delete>0:\r\n",
        "    print(f'Очищено фраз: {delete}')\r\n",
        "  return np.array(Ypred_res), np.array(paragraphs_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi9ZeWjEBM18"
      },
      "source": [
        "# **Reading input data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUS-Cv30ekv9"
      },
      "source": [
        "## Reading and verifying data (XML parsing). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvLTZMldti8v"
      },
      "source": [
        "### Training sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7QVhK0ce-6A"
      },
      "source": [
        "paragraphs_amount = 3363\n",
        "ps_train = load_xml('/content/drive/My Drive/Базы/NLP/База_словарей_train_6.xml', paragraphs_amount)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG4RmqkYUJdJ"
      },
      "source": [
        "paragraphs_train, tags_train, train_colors, max_paragraph_len, max_paragraph_len_ind, exception_array, semantics_train  = pars_xml(ps_train)\n",
        "paragraphs_amount = len(paragraphs_train)  # обновляем длину массива на случай наличия в исходной базе пустых строк\n",
        "paragraphs_amount, exception_array # контрольный вывод на размера принятых данных и массива индексов пустых строк"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEHXJYBWfAXC"
      },
      "source": [
        "# # Визуальный контроль правильности разбора файла\n",
        "# for i, elem in enumerate(paragraphs_train):\n",
        "#   print(i+1, elem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__gn6vZHQfnT"
      },
      "source": [
        "train_yes = 0\n",
        "train_no = 0\n",
        "for elem in semantics_train:\n",
        "  if elem[0]==1:\n",
        "    train_yes += 1\n",
        "  else:\n",
        "    train_no += 1\n",
        "print(f\"train: yes={train_yes}, no={train_no}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehD-V4zfe-6H"
      },
      "source": [
        "if len(train_colors) > colors_count:\n",
        "  print('В документе цветов разметки больше допустимого')\n",
        "# train_colors # выведем все цвета, которые мы встретили"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtqyJZCuwNi-"
      },
      "source": [
        "print(len(paragraphs_train))\n",
        "print(len(tags_train))\n",
        "\n",
        "print('Максимальная длина предложения — max_paragraph_len =',max_paragraph_len)\n",
        "print('Индекс предложения с максимальной длиной — max_paragraph_len_ind =',max_paragraph_len_ind)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0nnj8jil4ad"
      },
      "source": [
        "# Корректируем максимальную длину предложения для использования U-net сеток\n",
        "max_paragraph_len = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlXDpMje2KGW"
      },
      "source": [
        "train_paragraphs_len = len(paragraphs_train)\n",
        "print('Длина обучающей выборки:', train_paragraphs_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG3LwhSqsRAP"
      },
      "source": [
        "**Извлекаем статистику по абзацам**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yxpLMEpoaJc"
      },
      "source": [
        "train_counter, train_count = draw_paragraphs_stat(paragraphs_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54-6xsxM27b6"
      },
      "source": [
        "**Во скольких фразах встречается тот или  иной класс**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7j0DS0EYJG7"
      },
      "source": [
        "class_stat(paragraphs_train, tags_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5qlEXQxY0mA"
      },
      "source": [
        "### The extraction of morphological features for lexemes from the array paragraphs_train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4NT-5DdT_Fw"
      },
      "source": [
        "morph_train = []\n",
        "for i, elem in enumerate(paragraphs_train):\n",
        "  row = []\n",
        "  for j,lexem in enumerate(elem):\n",
        "    row.append(morphyus(lexem))\n",
        "  morph_train.append(row)\n",
        "  # print(f'{i+1} done {j+1}!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CLZPHPwMP0r"
      },
      "source": [
        "# for mrph in morph_train[1999]:\n",
        "#   print(mrph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-YvaF0CKWMl"
      },
      "source": [
        "# Статистика по длинам морфологических описаний для лексем \n",
        "k = 0\n",
        "word = ''\n",
        "index = -1\n",
        "a = set()\n",
        "for i, elem in enumerate(morph_train):\n",
        "  for j,lexem in enumerate(elem):\n",
        "    dlina = len(lexem)\n",
        "    a.add(dlina)\n",
        "    if dlina>k:\n",
        "      index = i\n",
        "      k = dlina\n",
        "      word = lexem[2]\n",
        "\n",
        "print(f'Лексема с максимально длинным морфологическим описанием: max(lexemdescription) = {k}; word = {word}; index(предложения) = {index}')\n",
        "print('Перечень всех вариантов для длины морфологического описания лексемы\\n',sorted(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euo_4NOAU5QP"
      },
      "source": [
        "# Составим массив нормальных форм слов параллельный массиву paragraphs_train\n",
        "# Составим массив частей речи параллельный массиву paragraphs_train\n",
        "# Составим массив падежей слов параллельный массиву paragraphs_train\n",
        "# Составим массив языковой принадлежности слов параллельный массиву paragraphs_train\n",
        "norms_train = []\n",
        "pos_train = []\n",
        "case_train = []\n",
        "lang_train = []\n",
        "for i, elem in enumerate(morph_train):\n",
        "  norms_sentence = []\n",
        "  pos_sentence = []\n",
        "  case_sentence = []\n",
        "  lang_sentence = []\n",
        "  for j,lexem in enumerate(elem):\n",
        "    len_descr = len(lexem)\n",
        "    lang_sentence.append(lexem[0])\n",
        "    norms_sentence.append(lexem[2])\n",
        "    # собрать все части речи у лексемы\n",
        "    pos = set()\n",
        "    for k in range(3, len_descr,13):\n",
        "      pos.add(lexem[k])\n",
        "    pos_sentence.append(\"\".join(list(pos)))\n",
        "    # собрать у лексемы все падежи и \n",
        "    # признаки формы глагола, причастия, деепричастия - GRND, PRTS, PRTF, INFN, VERB\n",
        "    case = set()\n",
        "    if lexem[3] in ['GRND', 'PRTS', 'PRTF', 'INFN', 'VERB']:\n",
        "      for k in range(4, len_descr,13):\n",
        "        for m in range(k, k+11):\n",
        "          case.add(lexem[m] if not lexem[m] in ['None'] else '')\n",
        "    else: \n",
        "      for k in range(6, len_descr,13):\n",
        "        case.add(lexem[k])\n",
        "    case_sentence.append(\"\".join(list(case)))\n",
        "  norms_train.append(norms_sentence)  \n",
        "  pos_train.append(pos_sentence)\n",
        "  case_train.append(case_sentence)\n",
        "  lang_train.append(lang_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK3U9wKadfNY"
      },
      "source": [
        "# for i, elem in enumerate(lang_train):\n",
        "#   print(i+1, elem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rBDS9VvXv_a"
      },
      "source": [
        "# for i, elem in enumerate(norms_train):\n",
        "#   print(i+1, elem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMu4Y_GeY7D_"
      },
      "source": [
        "# for i, elem in enumerate(pos_train):\n",
        "#   print(i+1, elem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DamN8ppc1Sg"
      },
      "source": [
        "# for i, elem in enumerate(case_train):\n",
        "#   print(i+1, elem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpOYDpNQZhvt"
      },
      "source": [
        "### Testing sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNZVFg00cR2T"
      },
      "source": [
        "test_paragraphs_amount = 703\n",
        "ps_test = load_xml('/content/drive/My Drive/Базы/NLP/База_словарей_test_6.xml', test_paragraphs_amount)\n",
        "print(len(ps_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u2KsjhsXqKd"
      },
      "source": [
        "paragraphs_test, tags_test, test_colors, max_test_paragraph_len, max_test_paragraph_len_ind, test_exception_array, semantics_test  = pars_xml(ps_test)\n",
        "test_paragraphs_amount = len(paragraphs_test)  # обновляем длину массива на случай наличия в исходной базе пустых строк\n",
        "test_paragraphs_amount, test_exception_array # контрольный вывод на размера принятых данных и массива индексов пустых строк"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iysXx80qVZ89"
      },
      "source": [
        "# # Визуальный контроль правильности разбора файла\n",
        "# for i, elem in enumerate(paragraphs_test):\n",
        "#   print(i+1, elem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvtjLFY5Kh30"
      },
      "source": [
        "test_yes = 0\n",
        "test_no = 0\n",
        "for elem in semantics_test:\n",
        "  if elem[0]==1:\n",
        "    test_yes += 1\n",
        "  else:\n",
        "    test_no += 1\n",
        "print(f\"train: yes={test_yes}, no={test_no}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sx9tDiKZhwd"
      },
      "source": [
        "if len(test_colors) > colors_count:\n",
        "  print('В документе цветов разметки больше допустимого')\n",
        "# test_colors # выведем все цвета, которые мы встретили"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndFEl2yFZhwh"
      },
      "source": [
        "print(len(paragraphs_test))\n",
        "print(len(tags_test))\n",
        "\n",
        "print('Максимальная длина предложения — max_test_paragraph_len =',max_test_paragraph_len)\n",
        "print('Индекс предложения с максимальной длиной — max_test_paragraph_len_ind =',max_test_paragraph_len_ind)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9uQTUuWZhwk"
      },
      "source": [
        "# Корректируем максимальную длину предложения для использования U-net сеток\n",
        "max_test_paragraph_len = max_paragraph_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsG2lGRlZhwo"
      },
      "source": [
        "**Извлекаем статистику по абзацам**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pk9vJ43rBDJ"
      },
      "source": [
        "test_counter, test_count = draw_paragraphs_stat(paragraphs_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASG-tAwY9X2-"
      },
      "source": [
        "**Во скольких фразах встречается тот или  иной класс**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPdBw9dPP2n6"
      },
      "source": [
        "class_stat(paragraphs_test, tags_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exHVu42xsbWr"
      },
      "source": [
        "### The extraction of morphological features for lexemes from the array paragraphs_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBlYXxvRsbWs"
      },
      "source": [
        "morph_test = []\n",
        "for i, elem in enumerate(paragraphs_test):\n",
        "  row = []\n",
        "  for j,lexem in enumerate(elem):\n",
        "    row.append(morphyus(lexem))\n",
        "  morph_test.append(row)\n",
        "  # print(f'{i+1} done {j+1}!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSYpmGQUsbWs"
      },
      "source": [
        "len(morph_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPqgYoVMsbWt"
      },
      "source": [
        "# for mrph in morph_test[665]:\n",
        "#   print(mrph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTbbNbYvsbWu"
      },
      "source": [
        "# Статистика по длинам морфологических описаний для лексем \n",
        "k_test = 0\n",
        "word_test = ''\n",
        "index_test = -1\n",
        "a_test = set()\n",
        "for i, elem in enumerate(morph_test):\n",
        "  for j,lexem in enumerate(elem):\n",
        "    dlina = len(lexem)\n",
        "    a.add(dlina)\n",
        "    if dlina>k_test:\n",
        "      index_test = i\n",
        "      k_test = dlina\n",
        "      word_test = lexem[2]\n",
        "\n",
        "print(f'Лексема с максимально длинным морфологическим описанием: max(lexemdescription) = {k_test}; word = {word_test}; index(предложения) = {index_test}')\n",
        "print('Перечень всех вариантов для длины морфологического описания лексемы\\n',sorted(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhss0nQCsbWu"
      },
      "source": [
        "# Составим массив нормальных форм слов параллельный массиву paragraphs_test\n",
        "# Составим массив частей речи параллельный массиву paragraphs_test\n",
        "# Составим массив падежей слов параллельный массиву paragraphs_test\n",
        "# Составим массив языковой принадлежности слов параллельный массиву paragraphs_test\n",
        "norms_test = []\n",
        "pos_test = []\n",
        "case_test = []\n",
        "lang_test = []\n",
        "for i, elem in enumerate(morph_test):\n",
        "  norms_sentence = []\n",
        "  pos_sentence = []\n",
        "  case_sentence = []\n",
        "  lang_sentence = []\n",
        "  for j,lexem in enumerate(elem):\n",
        "    len_descr = len(lexem)\n",
        "    lang_sentence.append(lexem[0])\n",
        "    norms_sentence.append(lexem[2])\n",
        "    # собрать все части речи у лексемы\n",
        "    pos = set()\n",
        "    for k in range(3, len_descr,13):\n",
        "      pos.add(lexem[k])\n",
        "    pos_sentence.append(\"\".join(list(pos)))\n",
        "    # собрать у лексемы все падежи и \n",
        "    # признаки формы глагола, причастия, деепричастия - GRND, PRTS, PRTF, INFN, VERB\n",
        "    case = set()\n",
        "    if lexem[3] in ['GRND', 'PRTS', 'PRTF', 'INFN', 'VERB']:\n",
        "      for k in range(4, len_descr,13):\n",
        "        for m in range(k, k+11):\n",
        "          case.add(lexem[m] if not lexem[m] in ['None'] else '')\n",
        "    else: \n",
        "      for k in range(6, len_descr,13):\n",
        "        case.add(lexem[k])\n",
        "    case_sentence.append(\"\".join(list(case)))\n",
        "  norms_test.append(norms_sentence)  \n",
        "  pos_test.append(pos_sentence)\n",
        "  case_test.append(case_sentence)\n",
        "  lang_test.append(lang_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXUHU3gKsbWu"
      },
      "source": [
        "# for i, elem in enumerate(lang_test):\n",
        "#   print(i+1, elem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jIOtctysbWv"
      },
      "source": [
        "# for i, elem in enumerate(norms_test):\n",
        "#   print(i+1, elem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l7-GgVtsbWy"
      },
      "source": [
        "# for i, elem in enumerate(pos_test):\n",
        "#   print(i+1, elem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtrVmdn4sbW0"
      },
      "source": [
        "# for i, elem in enumerate(case_test):\n",
        "#   print(i+1, elem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bWz3JCScVln"
      },
      "source": [
        "# **Step 1. Text Classification** on the Parts of Speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hWix19rBuc4"
      },
      "source": [
        "## Preparing a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ruR57sMDW9d"
      },
      "source": [
        "Объединим обучающий и тестовый массивы, чтобы воспользоваться общим токенизатором для нескольких входов с данными о pos, case, и т.д."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v2qo_4mDW9d"
      },
      "source": [
        "full_pcln = pos_train + pos_test + case_train + case_test + lang_train + lang_test + norms_train + norms_test\n",
        "full_semantics = semantics_train + semantics_test\n",
        "print('Суммарная размерность входных данных:', len(full_pcln))\n",
        "print('Размерность выходных данных:', len(full_semantics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWLao6tZDW9e"
      },
      "source": [
        "full_pcln_together = [' '.join(sequence) for sequence in full_pcln]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc6JUOpvDW9e"
      },
      "source": [
        "# full_pcln_together[max_paragraph_len_ind]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vx7yx2hDW9f"
      },
      "source": [
        "# full_semantics[max_paragraph_len_ind]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgaMxrvyDW9f"
      },
      "source": [
        "num_words = 10000 \n",
        "# 2000 из расчёта: \n",
        "# 550 единиц на морфологические признаки и на наименования частей речи\n",
        "# 1450 единиц на наиболее частотные лексемы из текста\n",
        "sent_len = max_paragraph_len \n",
        "tokenizer = Tokenizer(num_words, filters=' \\t\\n', oov_token='<UNK>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf8wQL3iDW9f"
      },
      "source": [
        "tokenizer.fit_on_texts(full_pcln_together)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cMiJkzQDW9f"
      },
      "source": [
        "# реальная длина словаря. Сравниваем со значением num_words\n",
        "'реальная длина словаря токенизатора', len(tokenizer.index_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09hWKNlJ9S01"
      },
      "source": [
        "**Токенизируем входные данные.** (Выходные данные у нас изначально представляют собой токены)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt9m4M0KDW9g"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(full_pcln_together)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF1ULP8fDW9g"
      },
      "source": [
        "Y = np.array(full_semantics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvxR-qcMA4j5"
      },
      "source": [
        "**Теперь разобъём X на два входных массива с данными о pos, cases, lang и norms размерностью paragraphs_amount + test_paragraphs_amount каждый**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFjuwVSF9nyl"
      },
      "source": [
        "L = paragraphs_amount + test_paragraphs_amount\n",
        "M = 2*L\n",
        "N = M+L\n",
        "X_pos = X[:L]\n",
        "X_case = X[L:M]\n",
        "X_lang = X[M:N]\n",
        "X_norms = X[N:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdvFQRnHDW9g"
      },
      "source": [
        "**Проверка входящих и исходящих массивов на соответствие их размерностей** \n",
        "\n",
        "(для выявления факта порчи данных функцией Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So7A2OFzDW9g"
      },
      "source": [
        "len(X_pos)==len(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWAcoXnW-UwL"
      },
      "source": [
        "len(X_case)==len(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNx9DOwFw9kq"
      },
      "source": [
        "len(X_lang)==len(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmOXwBs0w-WL"
      },
      "source": [
        "len(X_norms)==len(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsuVOZd8HjHr"
      },
      "source": [
        "# print(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l68cCZCHoOi"
      },
      "source": [
        "# print(X_pos[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpaD8qhW-X-y"
      },
      "source": [
        "# print(X_case[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkivHFDRxTdo"
      },
      "source": [
        "# print(X_lang[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_88ETpsYxUIy"
      },
      "source": [
        "# print(X_norms[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsQhJuwUDW9h"
      },
      "source": [
        "**Выравнивание массивов до максимального размера абзаца**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_kzgYYfDW9h"
      },
      "source": [
        "X_pos = pad_sequences(X_pos, sent_len, padding='post', truncating='post') # заполнение в конце и обрезка с конца предложения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LinxOtz8-i3I"
      },
      "source": [
        "X_case = pad_sequences(X_case, sent_len, padding='post', truncating='post') # заполнение в конце и обрезка с конца предложения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmIy3bMTxqu9"
      },
      "source": [
        "X_lang = pad_sequences(X_lang, sent_len, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml6B1eHdxc6q"
      },
      "source": [
        "X_norms = pad_sequences(X_norms, sent_len, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuNGhSWjDW9h"
      },
      "source": [
        "X_pos.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1a_wfxB-nSe"
      },
      "source": [
        "X_case.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsrqSAVixitm"
      },
      "source": [
        "X_lang.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy-wPHlbxgY1"
      },
      "source": [
        "X_norms.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0UA0wUTDW9h"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uNbMth5DW9h"
      },
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "Xtrain_pos, Xtest_pos, Ytrain, Ytest = train_test_split(X_pos, Y, train_size=train_paragraphs_len, shuffle=False) # Не перемешиваем данные и берём целую часть на тренировочную выборку"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0w8CVbe-snb"
      },
      "source": [
        "Xtrain_case = X_case[:train_paragraphs_len]\n",
        "Xtest_case = X_case[train_paragraphs_len:]\n",
        "\n",
        "Xtrain_lang = X_lang[:train_paragraphs_len]\n",
        "Xtest_lang = X_lang[train_paragraphs_len:]\n",
        "\n",
        "Xtrain_norms = X_norms[:train_paragraphs_len]\n",
        "Xtest_norms = X_norms[train_paragraphs_len:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeeYC9ZI_C4G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDuu6yETDW9h"
      },
      "source": [
        "### Total for training and testing samples we have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opbx6okpDW9h"
      },
      "source": [
        "print('Xtrain_pos.shape=',Xtrain_pos.shape)\n",
        "print('Xtest_pos.shape=',Xtest_pos.shape)\n",
        "print('Xtrain_case.shape=',Xtrain_case.shape)\n",
        "print('Xtest_case.shape=',Xtest_case.shape)\n",
        "\n",
        "print('Xtrain_lang.shape=',Xtrain_lang.shape)\n",
        "print('Xtest_lang.shape=',Xtest_lang.shape)\n",
        "print('Xtrain_norms.shape=',Xtrain_norms.shape)\n",
        "print('Xtest_norms.shape=',Xtest_norms.shape)\n",
        "\n",
        "print('Ytrain.shape=',Ytrain.shape)\n",
        "print('Ytest.shape=',Ytest.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugo4-mgf_Wxs"
      },
      "source": [
        "# Xtrain_pos[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "minn7lIVUvCw"
      },
      "source": [
        "# Xtrain_case[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXj3bUO8ykyM"
      },
      "source": [
        "# Xtrain_lang[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-tK9bAzylSL"
      },
      "source": [
        "# Xtrain_norms[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BuuKXalUyy5"
      },
      "source": [
        "# Ytrain[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-H265oSr-RE"
      },
      "source": [
        "## Recurrent neural network model (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xdBDgrnL6Zq"
      },
      "source": [
        "embedding_size = 100\n",
        "\n",
        "# параллельная архитектура\n",
        "input_pos = Input((sent_len,))\n",
        "x_pos = Embedding(num_words, embedding_size)(input_pos)\n",
        "x_pos = Conv1D(64, 3, padding='same', name='input1_conv1')(x_pos)\n",
        "x_pos = BatchNormalization()(x_pos)\n",
        "x_pos = Activation('relu')(x_pos)\n",
        "x_pos = Dropout(0.2)(x_pos)\n",
        "\n",
        "input_case = Input((sent_len,))\n",
        "x_case = Embedding(num_words, embedding_size)(input_case)\n",
        "x_case = Conv1D(64, 3, padding='same', name='input2_conv1')(x_case)\n",
        "x_case = BatchNormalization()(x_case)\n",
        "x_case = Activation('relu')(x_case)\n",
        "x_case = Dropout(0.2)(x_case)\n",
        "\n",
        "input_lang = Input((sent_len,))\n",
        "x_lang = Embedding(num_words, embedding_size)(input_lang)\n",
        "x_lang = Conv1D(64, 3, padding='same', name='input3_conv1')(x_lang)\n",
        "x_lang = BatchNormalization()(x_lang)\n",
        "x_lang = Activation('relu')(x_lang)\n",
        "x_lang = Dropout(0.2)(x_lang)\n",
        "\n",
        "input_norms = Input((sent_len,))\n",
        "x_norms = Embedding(num_words, embedding_size)(input_norms)\n",
        "x_norms = Conv1D(64, 3, padding='same', name='input4_conv1')(x_norms)\n",
        "x_norms = BatchNormalization()(x_norms)\n",
        "x_norms = Activation('relu')(x_norms)\n",
        "x_norms = Dropout(0.2)(x_norms)\n",
        "\n",
        "x = concatenate([x_pos, x_case, x_lang, x_norms])\n",
        "\n",
        "# последовательная архитектура\n",
        "# x_pos = Embedding(num_words, embedding_size)(input_pos)\n",
        "# x_pos = BatchNormalization()(x_pos)\n",
        "# x_pos = LSTM(128, return_sequences=True)(x_pos)\n",
        "# x_pos = Dropout(0.2)(x_pos)\n",
        "\n",
        "# input_case = Input((sent_len,))\n",
        "# x_case = Embedding(num_words, embedding_size)(input_case)\n",
        "\n",
        "# input_lang = Input((sent_len,))\n",
        "# x_lang = Embedding(num_words, embedding_size)(input_lang)\n",
        "\n",
        "# input_norms = Input((sent_len,))\n",
        "# x_norms = Embedding(num_words, embedding_size)(input_norms)\n",
        "\n",
        "\n",
        "# x = concatenate([x_pos, x_case])\n",
        "# x = BatchNormalization()(x)\n",
        "# x = LSTM(128, return_sequences=True)(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "\n",
        "# x = concatenate([x, x_lang])\n",
        "# x = BatchNormalization()(x)\n",
        "# x = LSTM(128, return_sequences=True)(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "\n",
        "# x = concatenate([x, x_norms])\n",
        "# x = BatchNormalization()(x)\n",
        "x = LSTM(128, return_sequences=True)(x)\n",
        "x = SpatialDropout1D(0.2)(x)\n",
        "x = Flatten()(x)\n",
        "# x = BatchNormalization()(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "modelLSTM = Model(inputs=[input_pos, input_case, input_lang, input_norms], outputs=output)\n",
        "\n",
        "#  'adam' 'rmsprop'   Adam\n",
        "modelLSTM.compile(loss='binary_crossentropy', metrics=[AUC(num_thresholds=150)], optimizer=RMSprop(lr=1e-4)) #\n",
        "\n",
        "# modelLSTM.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvoK433uLmj5"
      },
      "source": [
        "'''\r\n",
        "  callback-функция раннего останова по достижению достаточного процента точности\r\n",
        "'''\r\n",
        "def stop_NNT(about):\r\n",
        "  print(about) \r\n",
        "  # https://www.tensorflow.org/guide/keras/custom_callback\r\n",
        "  modelLSTM.stop_training = True\r\n",
        "\r\n",
        "def on_epoch_end(epoch, logs):\r\n",
        "  if (epoch > 10):\r\n",
        "    precision = logs.get('val_auc')\r\n",
        "    precision_train = logs.get('auc')\r\n",
        "    if precision_train > 0.999 and precision > 0.996:\r\n",
        "      stop_NNT(\"\\nОбучение приостановлено при достаточном значении точности . val_auc = {}\".format(round(precision, 3)))      \r\n",
        "\r\n",
        "early_stop = tf.keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox55diZ9FyFQ"
      },
      "source": [
        "plot_model(modelLSTM, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtcPNMhYL6Z3"
      },
      "source": [
        "historyLSTM = modelLSTM.fit(x=[Xtrain_pos, Xtrain_case, Xtrain_lang, Xtrain_norms], y=Ytrain, \n",
        "                            batch_size=96, epochs=61, \n",
        "                            validation_data=([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms], Ytest), \n",
        "                            callbacks=[early_stop])\n",
        "                    \n",
        "modelLSTM.save( '/content/drive/My Drive/tmp/modelLSTM_classification(production).h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdjTEJNmL6Z7"
      },
      "source": [
        "# график прогрессии обучения нейросети в Matplotlib\n",
        "plt.plot(historyLSTM.history['auc'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyLSTM.history['val_auc'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iphoNPFEL6ab"
      },
      "source": [
        "**Проверка качества работы нейронной сети**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7CkcmElL6ad"
      },
      "source": [
        "Y_hat = modelLSTM.predict([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms])\n",
        "'Размерность предсказаний', Y_hat.shape, 'Размерность тестовых данных', Ytest.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWihKqnLZKbt"
      },
      "source": [
        "y_coincidence = 0\n",
        "y_unrecognized = 0\n",
        "y_contrived = 0\n",
        "for i, pred in enumerate(Y_hat):\n",
        "  b = Ytest[i][0]\n",
        "  a = round(pred[0])\n",
        "  # print(i+1, a, b, paragraphs_test[i])\n",
        "  if a+b>1.5:\n",
        "    y_coincidence += 1\n",
        "  elif a+b<0.5:\n",
        "    y_coincidence += 1\n",
        "  else:\n",
        "    if a>0.5:\n",
        "      y_contrived += 1\n",
        "    else:\n",
        "      y_unrecognized += 1\n",
        "\n",
        "print(f\"\\nРаспознано: {y_coincidence}; Нераспознано: {y_unrecognized}; Надумано: {y_contrived}\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZJe9MLgL6ae"
      },
      "source": [
        "ROC-кривая (англ. receiver operating characteristic, рабочая характеристика приёмника) — график, позволяющий оценить качество бинарной классификации, отображает соотношение между долей объектов от общего количества носителей признака, верно классифицированных как несущие признак (англ. true positive rate, TPR, называемой чувствительностью алгоритма классификации), и долей объектов от общего количества объектов, не несущих признака, ошибочно классифицированных как несущие признак (англ. false positive rate, FPR, величина 1-FPR называется специфичностью алгоритма классификации) при варьировании порога решающего правила.\n",
        "\n",
        "https://ru.wikipedia.org/wiki/ROC-%D0%BA%D1%80%D0%B8%D0%B2%D0%B0%D1%8F"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKFGwpz-L6af"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "\n",
        "num_classes = 1 #узнаем, сколько всего классов\n",
        "for i in range(num_classes): #проходимся по всем классам\n",
        "  fpr, tpr, thresholds = roc_curve(Ytest[:, i], Y_hat[:, i]) #получаем roc-кривую для очередного класса\n",
        "  plt.plot(fpr, tpr, label='{} класс - {}, AUC={}'.format(i, dic_color[i], AUC()(Ytest[:, i], Y_hat[:, i]))) #отрисовываем кривую и подписываем ее площадь\n",
        "\n",
        "avg_fpr, avg_tpr, thresholds = roc_curve(Ytest.flatten(), Y_hat.flatten()) #получаем micro-average (общую) кривую roc\n",
        "plt.plot(avg_fpr, avg_tpr, 'mD',label='micro-average, AUC={}'.format(AUC()(Ytest, Y_hat)))#отрисовываем кривую и подписываем ее площадь\n",
        "plt.plot([0,1], [0,1], 'b-.') #обозначим границу качественной работы нейросети\n",
        "plt.fill_between([0,1], [0,1], label='Зона некачестванного распознавания', color='pink', alpha=0.3) #обозначим границу некачественной работы нейросети\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zXv_Ad7dsZJ"
      },
      "source": [
        "## Fetching arrays for the next step in extracting definitional structures in phrases with expected semantic status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r830PuChdrPi"
      },
      "source": [
        "pos_train_2 = []\n",
        "case_train_2 = []\n",
        "lang_train_2 = []\n",
        "norms_train_2 = []\n",
        "tags_train_2 = []\n",
        "paragraphs_train_2 = []\n",
        "for i, elem in enumerate(semantics_train):\n",
        "  if elem[0]==1:\n",
        "    pos_train_2.append(pos_train[i])\n",
        "    case_train_2.append(case_train[i])\n",
        "    lang_train_2.append(lang_train[i])\n",
        "    norms_train_2.append(norms_train[i])\n",
        "    paragraphs_train_2.append(paragraphs_train[i])\n",
        "    tags_train_2.append(tags_train[i])\n",
        "\n",
        "print(f'Обучающие массивы (с размерностью {train_yes})', \n",
        "      len(pos_train_2), len(case_train_2), \n",
        "      len(lang_train_2), len(norms_train_2), \n",
        "      len(tags_train_2), len(paragraphs_train_2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZfz-N2qgB-E"
      },
      "source": [
        "pos_test_2 = []\n",
        "case_test_2 = []\n",
        "lang_test_2 = []\n",
        "norms_test_2 = []\n",
        "tags_test_2 = []\n",
        "paragraphs_test_2 = []\n",
        "for i, elem in enumerate(Y_hat):\n",
        "  if round(elem[0])>0:\n",
        "    pos_test_2.append(pos_test[i])\n",
        "    case_test_2.append(case_test[i])\n",
        "    lang_test_2.append(lang_test[i])\n",
        "    norms_test_2.append(norms_test[i])\n",
        "    paragraphs_test_2.append(paragraphs_test[i])\n",
        "    tags_test_2.append(tags_test[i])\n",
        "\n",
        "print(f'Тренировочные массивы (с размерностью {test_yes}):',\n",
        "      len(pos_test_2), len(case_test_2), \n",
        "      len(lang_test_2), len(norms_test_2), \n",
        "      len(tags_test_2), len(paragraphs_test_2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0tvvy1THBj"
      },
      "source": [
        "# **Step 2. Segmentation of text** on the Parts of Speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qQTEy0vh8CQ"
      },
      "source": [
        "## Preparing a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0i9mlHIxjN9"
      },
      "source": [
        "Объединим обучающий и тестовый массивы, чтобы воспользоваться общим токенизатором."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5WmvWfoyg-b"
      },
      "source": [
        "# Здесь для обучения оставим обучающую выборку в полном объёме, \n",
        "# а из тестовой возьмём только то, что выбрала нейронка на предыдущем шаге\n",
        "all_pcln = pos_train + pos_test_2 + case_train + case_test_2 + \\\n",
        "           lang_train + lang_test_2 + norms_train + norms_test_2 \n",
        "all_tags = tags_train + tags_test_2\n",
        "all_paragraphs = paragraphs_train + paragraphs_test_2\n",
        "train_test_paragraphs_amount_2 = len(all_paragraphs)\n",
        "\n",
        "print('Суммарная размерность входных данных:', len(all_pcln))\n",
        "print('Размерность выходных данных:', len(all_tags))\n",
        "print('Размерность датасета:', len(all_paragraphs))\n",
        "\n",
        "print(f'train_test_paragraphs_amount_2 = {train_test_paragraphs_amount_2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkzlZG13zQHr"
      },
      "source": [
        "all_pcln_together = [' '.join(sequence) for sequence in all_pcln]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f85v44o83tTU"
      },
      "source": [
        "# len(all_pcln_together)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9McUbxZjzW01"
      },
      "source": [
        "# all_pcln_together[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6oJ_dC-zcl-"
      },
      "source": [
        "all_tags_together = [' '.join(tag) for tag in all_tags]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4FiUOW7zj5n"
      },
      "source": [
        "# all_tags_together[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZnCt24_ku1Y"
      },
      "source": [
        "**Токенизируем входные и выходные данные.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE0Vc6Zu06YA"
      },
      "source": [
        "X_2 = tokenizer.texts_to_sequences(all_pcln_together)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqwEgXhu06YX"
      },
      "source": [
        "Y_2 = tag_tokenizer.texts_to_sequences(all_tags_together)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja1wozrncXKo"
      },
      "source": [
        "# Y_2[13]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXQEE_yeku1Z"
      },
      "source": [
        "**Теперь разобъём X на два входных массива с данными о pos и cases размерностью paragraphs_amount + test_paragraphs_amount каждый**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT2oNQFyku1Z"
      },
      "source": [
        "L = train_test_paragraphs_amount_2\n",
        "M = 2*L\n",
        "N = M+L\n",
        "X_2_pos = X_2[:L]\n",
        "X_2_case = X_2[L:M]\n",
        "X_2_lang = X_2[M:N]\n",
        "X_2_norms = X_2[N:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTaSRv1SVtww"
      },
      "source": [
        "**Проверка входящих и исходящих массивов на соответствие их размерностей** \n",
        "\n",
        "(для выявления факта порчи данных функцией Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG3vDBirlmfv"
      },
      "source": [
        "len(X_2_pos)==len(Y_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuzn9P4qlmfz"
      },
      "source": [
        "len(X_2_case)==len(Y_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RI-5ZydrO7G"
      },
      "source": [
        "len(X_2_lang)==len(Y_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9ZRaV3crPai"
      },
      "source": [
        "len(X_2_norms)==len(Y_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4C9G1ACWq9O"
      },
      "source": [
        "# Проверка на согласованность входных и выходных данных\n",
        "for i, elem in enumerate(X_2_pos):\n",
        "  if len(Y_2[i]) != len(X_2_pos[i]):\n",
        "    print('X_2_pos[{}] ='.format(i),X_2_pos[i])\n",
        "    print('Y_2[{}] ='.format(i),Y_2[i])\n",
        "    # Если все массивы равны, то здесь  ничего не должно печататься\n",
        "  else:\n",
        "    pass\n",
        "    # print('len(X_2_pos[{}]) ='.format(i),len(Y_2[i]))\n",
        "  if len(Y_2[i]) != len(X_2_case[i]):\n",
        "    print('X_2_case[{}] ='.format(i),X_2_case[i])\n",
        "    print('Y_2[{}] ='.format(i),Y_2[i])\n",
        "    # Если все массивы равны, то здесь  ничего не должно печататься\n",
        "  else:\n",
        "    pass\n",
        "    # print('len(X_2_case[{}]) ='.format(i),len(Y_2[i]))\n",
        "  if len(Y_2[i]) != len(X_2_lang[i]):\n",
        "    print('X_2_lang[{}] ='.format(i),X_2_lang[i])\n",
        "    print('Y_2[{}] ='.format(i),Y_2[i])\n",
        "    # Если все массивы равны, то здесь  ничего не должно печататься\n",
        "  else:\n",
        "    pass\n",
        "    # print('len(X_2_lang[{}]) ='.format(i),len(Y_2[i]))\n",
        "  if len(Y_2[i]) != len(X_2_norms[i]):\n",
        "    print('X_2_norms[{}] ='.format(i),X_2_norms[i])\n",
        "    print('Y_2[{}] ='.format(i),Y_2[i])\n",
        "    # Если все массивы равны, то здесь  ничего не должно печататься\n",
        "  else:\n",
        "    pass\n",
        "    # print('len(X_2_norms[{}]) ='.format(i),len(Y_2[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlGw1JPNWNcb"
      },
      "source": [
        "**Выравнивание массивов до максимального размера абзаца**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLvxcpLlmWY_"
      },
      "source": [
        "X_2_pos = pad_sequences(X_2_pos, sent_len, padding='pre', truncating='post') # заполнение в начале и обрезка с конца предложения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "940qFwO4mWZA"
      },
      "source": [
        "X_2_case = pad_sequences(X_2_case, sent_len, padding='pre', truncating='post') # заполнение в начале и обрезка с конца предложения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC6PDxMSr2EJ"
      },
      "source": [
        "X_2_lang = pad_sequences(X_2_lang, sent_len, padding='pre', truncating='post') # заполнение в начале и обрезка с конца предложения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fI47Yclr1z9"
      },
      "source": [
        "X_2_norms = pad_sequences(X_2_norms, sent_len, padding='pre', truncating='post') # заполнение в начале и обрезка с конца предложения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5730kxrc06YJ"
      },
      "source": [
        "X_2_pos.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFIdG9HLme0A"
      },
      "source": [
        "X_2_case.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VozuGO6jrqQZ"
      },
      "source": [
        "X_2_lang.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iykb50mnrsSs"
      },
      "source": [
        "X_2_norms.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e_5mLwKVrK6"
      },
      "source": [
        "Y_2 = pad_sequences(Y_2, sent_len, value=1, padding='pre', truncating='post') # заполнение в начале и обрезка с конца предложения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mReWsYm06Yb"
      },
      "source": [
        "Y_2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA44evFJ06Ye"
      },
      "source": [
        "Y_2 = to_categorical(Y_2, colors_count+1)  # см. лекцию \"Занятие № 6 Углубленный курс по текстам\" https://youtu.be/Nfw7ZY-WzS8?t=1533\n",
        "Y_2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1rNkXVX06Ym"
      },
      "source": [
        "Y_2 = Y_2[:, :, 1:]  # см. лекцию \"Занятие № 6 Углубленный курс по текстам\" https://youtu.be/Nfw7ZY-WzS8?t=1533\n",
        "Y_2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueFossyeCz70"
      },
      "source": [
        "print(colorize_the_text(all_paragraphs[13], Y_2[13], 0, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxNRC3SOo9GP"
      },
      "source": [
        "Далее заменяем массивы из первого этапа по классификации предложений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DcThCk3nLmY"
      },
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "Xtrain_pos, Xtest_pos, Ytrain, Ytest = train_test_split(X_2_pos, Y_2, train_size=train_paragraphs_len, shuffle=False) # Не перемешиваем данные и берём целую часть на тренировочную выборку"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6eedXvEnLmZ"
      },
      "source": [
        "Xtrain_case = X_2_case[:train_paragraphs_len]\n",
        "Xtest_case = X_2_case[train_paragraphs_len:]\n",
        "\n",
        "Xtrain_lang = X_2_lang[:train_paragraphs_len]\n",
        "Xtest_lang = X_2_lang[train_paragraphs_len:]\n",
        "\n",
        "Xtrain_norms = X_2_norms[:train_paragraphs_len]\n",
        "Xtest_norms = X_2_norms[train_paragraphs_len:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YuzBTdbtyFD"
      },
      "source": [
        "### Total for training and testing samples we have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ouuv1n5oyjW"
      },
      "source": [
        "# Новые размерности уже \"только\" для массивов с определительными структурами, \n",
        "# с учетом вычисленных на этапе классификации предложений\n",
        "\n",
        "print('Xtrain_pos.shape=',Xtrain_pos.shape)\n",
        "print('Xtest_pos.shape=',Xtest_pos.shape)\n",
        "print('Xtrain_case.shape=',Xtrain_case.shape)\n",
        "print('Xtest_case.shape=',Xtest_case.shape)\n",
        "print('Xtrain_lang.shape=',Xtrain_lang.shape)\n",
        "print('Xtest_lang.shape=',Xtest_lang.shape)\n",
        "print('Xtrain_norms.shape=',Xtrain_norms.shape)\n",
        "print('Xtest_norms.shape=',Xtest_norms.shape)\n",
        "print('Ytrain.shape=',Ytrain.shape)\n",
        "print('Ytest.shape=',Ytest.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lc3E7AtoyjZ"
      },
      "source": [
        "# Xtrain_pos[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JeXvUTHoyja"
      },
      "source": [
        "# Xtrain_case[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOHtlcussesc"
      },
      "source": [
        "# Xtrain_lang[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzDDQwWgsfJL"
      },
      "source": [
        "# Xtrain_norms[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VrmJSLuoyja"
      },
      "source": [
        "# Ytrain[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfpdJqF6oWL8"
      },
      "source": [
        "# Пример окраски размеченного текста\n",
        "print(colorize_the_text(paragraphs_test_2[0], Ytest[0], max_paragraph_len, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg42wHw3aBXL"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX7UdQfrbjPi"
      },
      "source": [
        "emb_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz9NJZsW6Eua"
      },
      "source": [
        "# '''\n",
        "#   Собственная функция метрики, обрабатывающая пересечение двух областей\n",
        "# '''\n",
        "# def dice_coef(y_true, y_pred):\n",
        "#     return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.) # Возвращаем площадь пересечения деленную на площадь объединения двух областей"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MjP0SFXdH7e"
      },
      "source": [
        "# model_dir = \"/content/drive/My Drive/tmp/\"\n",
        "\n",
        "# '''\n",
        "#   callback-функция сохранения на диск лучшей модели в процессе обучения\n",
        "#   запись осуществляется на google-диск\n",
        "# '''\n",
        "# callbacks_list = [\n",
        "#     tf.keras.callbacks.ModelCheckpoint(\n",
        "#         filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
        "#         monitor=\"loss\",\n",
        "#         save_best_only=True,\n",
        "#         save_weights_only=True,\n",
        "#         mode='min',\n",
        "#         verbose=1\n",
        "#     )\n",
        "# ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afTSNoMYeMqJ"
      },
      "source": [
        "### **Recurrent neural network GRU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvDohHpAkost"
      },
      "source": [
        "#### Algorithm: Embedding + Bidirectional(GRU) + GRU , batch_size = 128, activation='softmax'\n",
        "\n",
        "*Последовательная архитектура*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iODBEshzeRKy"
      },
      "source": [
        "input_pos = Input((sent_len,))\n",
        "x_pos = Embedding(num_words, emb_size)(input_pos)\n",
        "x_pos = BatchNormalization()(x_pos)\n",
        "x_pos = Bidirectional(GRU(emb_size, return_sequences=True))(x_pos)\n",
        "x_pos = Dropout(0.2)(x_pos)\n",
        "\n",
        "input_case = Input((sent_len,))\n",
        "x_case = Embedding(num_words, emb_size)(input_case)\n",
        "\n",
        "input_lang = Input((sent_len,))\n",
        "x_lang = Embedding(num_words, emb_size)(input_lang)\n",
        "\n",
        "input_norms = Input((sent_len,))\n",
        "x_norms = Embedding(num_words, emb_size)(input_norms)\n",
        "\n",
        "x = concatenate([x_pos, x_case])\n",
        "x = BatchNormalization()(x)\n",
        "x = GRU(emb_size * 2, return_sequences=True)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = concatenate([x, x_lang])\n",
        "x = BatchNormalization()(x)\n",
        "x = GRU(emb_size * 2, return_sequences=True)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = concatenate([x, x_norms])\n",
        "x = BatchNormalization()(x)\n",
        "x = GRU(emb_size * 2, return_sequences=True)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "output = Dense(colors_count, activation='softmax')(x)\n",
        "\n",
        "modelEGRU = Model(inputs=[input_pos, input_case, input_lang, input_norms], outputs=output)\n",
        "\n",
        "modelEGRU.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# modelEGRU.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=[dice_coef, AUC(num_thresholds=150)])\n",
        "\n",
        "plot_model(modelEGRU, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE0BmpYCizoz"
      },
      "source": [
        "historyEGRU = modelEGRU.fit(x=[Xtrain_pos, Xtrain_case, Xtrain_lang, Xtrain_norms], y=Ytrain,\n",
        "                            epochs=100, batch_size=128,\n",
        "                            validation_data=([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms], Ytest))\n",
        "                    \n",
        "modelEGRU.save( '/content/drive/My Drive/tmp/modelEGRU_consistent(production).h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eohtEuZ75UOG"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyEGRU.history['loss'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyEGRU.history['val_loss'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB-UeyIphMUY"
      },
      "source": [
        "**Проверка тренировочной выборки (насколько хорошо выучены примеры)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArQoQjSBg7S3"
      },
      "source": [
        "# YpredEGRU_train = modelEGRU.predict([Xtrain_pos, Xtrain_case, Xtrain_lang, Xtrain_norms])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lDDkZ5Cg7S7"
      },
      "source": [
        "# for i, el in enumerate(paragraphs_train):\n",
        "#   print(f'{i+1}.')\n",
        "#   print(colorize_the_text(el, Ytrain[i], 0, 'pre', sent_len))\n",
        "#   print(colorize_the_text(el, YpredEGRU_train[i], 0, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jl5jgEiNNWZ"
      },
      "source": [
        "*Чтобы память не забивать, результат 100%-го совпадения предсказаний с разметкой обучающей выборки удалён из текста*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO92NflnN7uB"
      },
      "source": [
        "**Проверка тестовой выборки (насколько хорошо обучена сеть)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9txMeBMKl6BW"
      },
      "source": [
        "YpredEGRU_test = modelEGRU.predict([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms])\n",
        "YpredEGRU_test, Ytest_res, paragraphs_test_res = cleansed_data(YpredEGRU_test, Ytest, paragraphs_test_2) # нехорошие данные здесь всё-таки подчищаем"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK1gMJeZnv3-"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам\n",
        "print(classification_report(Ytest_res.argmax(axis=-1).flatten(), \n",
        "                            YpredEGRU_test.argmax(axis=-1).flatten(), \n",
        "                            target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI0-UgJiuZMb"
      },
      "source": [
        "# # выбираем конкретную фразу из тестовой выборки\n",
        "# i = 254 - 1\n",
        "# print(f'{i}.')\n",
        "# print(colorize_the_text(paragraphs_test_res[i], Ytest_res[i], 0, 'pre', sent_len))\n",
        "# print(colorize_the_text(paragraphs_test_res[i], YpredEGRU_test[i], 0, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeoeUCb-t5I4"
      },
      "source": [
        "# # воспроизводим все фразы из тестовой выборки\n",
        "# for i, el in enumerate(paragraphs_test_res):\n",
        "#   print(f'{i+1}.')\n",
        "#   print(colorize_the_text(el, Ytest_res[i], 0, 'pre', sent_len))\n",
        "#   print(colorize_the_text(el, YpredEGRU_test[i], 0, 'pre', sent_len))\n",
        "\n",
        "# # На github не отображается расцветка текста\n",
        "# # Чтобы её посмотреть, воспользуйтесь ссылкой на https://colab.research.google.com/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsIIMxW5crfv"
      },
      "source": [
        "#### Algorithm: Embedding + Bidirectional(GRU) + GRU , batch_size = 128, activation='softmax'\n",
        "\n",
        "*Параллельная архитектура*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajKr37_-crfv"
      },
      "source": [
        "# параллельная архитектура\n",
        "input_pos = Input((sent_len,))\n",
        "x_pos = Embedding(num_words, emb_size)(input_pos)\n",
        "x_pos = BatchNormalization()(x_pos)\n",
        "x_pos = Bidirectional(GRU(emb_size, return_sequences=True))(x_pos)\n",
        "x_pos = Activation('relu')(x_pos)\n",
        "x_pos = Dropout(0.2)(x_pos)\n",
        "\n",
        "input_case = Input((sent_len,))\n",
        "x_case = Embedding(num_words, embedding_size)(input_case)\n",
        "x_case = BatchNormalization()(x_case)\n",
        "x_case = Bidirectional(GRU(emb_size, return_sequences=True))(x_case)\n",
        "x_case = Activation('relu')(x_case)\n",
        "x_case = Dropout(0.2)(x_case)\n",
        "\n",
        "input_lang = Input((sent_len,))\n",
        "x_lang = Embedding(num_words, embedding_size)(input_lang)\n",
        "x_lang = Bidirectional(GRU(emb_size, return_sequences=True))(x_lang)\n",
        "x_lang = BatchNormalization()(x_lang)\n",
        "x_lang = Activation('relu')(x_lang)\n",
        "x_lang = Dropout(0.2)(x_lang)\n",
        "\n",
        "input_norms = Input((sent_len,))\n",
        "x_norms = Embedding(num_words, embedding_size)(input_norms)\n",
        "x_norms = Bidirectional(GRU(emb_size, return_sequences=True))(x_norms)\n",
        "x_norms = BatchNormalization()(x_norms)\n",
        "x_norms = Activation('relu')(x_norms)\n",
        "x_norms = Dropout(0.2)(x_norms)\n",
        "\n",
        "x = concatenate([x_pos, x_case, x_lang, x_norms])\n",
        "\n",
        "x = GRU(emb_size, return_sequences=True)(x)\n",
        "# x = BatchNormalization()(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "output = Dense(colors_count, activation='softmax')(x)\n",
        "\n",
        "modelEGRUpar = Model(inputs=[input_pos, input_case, input_lang, input_norms], outputs=output)\n",
        "\n",
        "modelEGRUpar.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# modelEGRUpar.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=[dice_coef, AUC(num_thresholds=150)])\n",
        "\n",
        "plot_model(modelEGRUpar, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tbise09crfw"
      },
      "source": [
        "historyEGRUpar = modelEGRUpar.fit(x=[Xtrain_pos, Xtrain_case, Xtrain_lang, Xtrain_norms], y=Ytrain,\n",
        "                            epochs=100, batch_size=128,\n",
        "                            validation_data=([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms], Ytest))\n",
        "                    \n",
        "modelEGRUpar.save( '/content/drive/My Drive/tmp/modelEGRU_parallel(production).h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo4gKiIN5O6U"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyEGRUpar.history['loss'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyEGRUpar.history['val_loss'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa1SGC3Mcrfx"
      },
      "source": [
        "**Проверка тренировочной выборки (насколько хорошо выучены примеры)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC-nqYEWcrfx"
      },
      "source": [
        "# YpredEGRU_train_par = modelEGRUpar.predict([Xtrain_pos, Xtrain_case, Xtrain_lang, Xtrain_norms])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB3XtVaScrfx"
      },
      "source": [
        "# for i, el in enumerate(paragraphs_train):\n",
        "#   print(f'{i+1}.')\n",
        "#   print(colorize_the_text(el, Ytrain[i], 0, 'pre', sent_len))\n",
        "#   print(colorize_the_text(el, YpredEGRU_train_par[i], 0, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkUa0hMGcrfx"
      },
      "source": [
        "*Чтобы память не забивать, результат 100%-го совпадения предсказаний с разметкой обучающей выборки удалён из текста*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFbfgSKxcrfx"
      },
      "source": [
        "**Проверка тестовой выборки (насколько хорошо обучена сеть)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riSiDgpJcrfy"
      },
      "source": [
        "YpredEGRU_test_par = modelEGRUpar.predict([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms])\n",
        "YpredEGRU_test_par, Ytest_par_res, paragraphs_test_par_res = cleansed_data(YpredEGRU_test_par, Ytest, paragraphs_test_2) # нехорошие данные здесь всё-таки подчищаем"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol41L7Wlcrfy"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам \n",
        "print(classification_report(Ytest_par_res.argmax(axis=-1).flatten(), \n",
        "                            YpredEGRU_test_par.argmax(axis=-1).flatten(), \n",
        "                            target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMniIn3_crfy"
      },
      "source": [
        "# # выбираем конкретную фразу из тестовой выборки\n",
        "# i = 2 - 1\n",
        "# print(f'{i}.')\n",
        "# print(colorize_the_text(paragraphs_test_par_res[i], Ytest_par_res[i], 0, 'pre', sent_len))\n",
        "# print(colorize_the_text(paragraphs_test_par_res[i], YpredEGRU_test_par[i], 0, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYfyNcyGcrfy"
      },
      "source": [
        "# # воспроизводим все фразы из тестовой выборки\n",
        "# for i, el in enumerate(paragraphs_test_par_res):\n",
        "#   print(f'{i+1}.')\n",
        "#   print(colorize_the_text(el, Ytest_par_res[i], 0, 'pre', sent_len))\n",
        "#   print(colorize_the_text(el, YpredEGRU_test_par[i], 0, 'pre', sent_len))\n",
        "\n",
        "# # На github не отображается расцветка текста\n",
        "# # Чтобы её посмотреть, воспользуйтесь ссылкой на https://colab.research.google.com/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S1HYdyZHIAA"
      },
      "source": [
        "### **One-dimensional convolution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-8OLgCqNPHW"
      },
      "source": [
        "#### Алгоритм: Embedding + Conv1D , batch_size = 16, activation='softmax'\n",
        "\n",
        "*Последовательная архитектура*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJEhXlIDlTgN"
      },
      "source": [
        "'''\n",
        "  Функция создания сети\n",
        "    Входные параметры:\n",
        "    - num_classes - количество классов для рапознавания\n",
        "    - sentence_len - длина предложения\n",
        "    - num_words - количество слов в словаре\n",
        "    - embedding_size - длина эмбединга\n",
        "'''\n",
        "def pseudoLinearSegmentationNet(\n",
        "      num_classes = 9,\n",
        "      sentence_len = 200,\n",
        "      num_words = 5000,\n",
        "      activation_function = 'softmax',\n",
        "      embedding_size = 500\n",
        "      ):\n",
        "    input_pos = Input((sentence_len,))\n",
        "    x_pos = Embedding(num_words, embedding_size)(input_pos)\n",
        "    x_pos = Conv1D(64, 3, padding='same', name='input1_conv1')(x_pos)\n",
        "    x_pos = BatchNormalization()(x_pos)\n",
        "    x_pos = Activation('relu')(x_pos)\n",
        "\n",
        "    input_case = Input((sentence_len,))\n",
        "    x_case = Embedding(num_words, embedding_size)(input_case)\n",
        "\n",
        "    input_lang = Input((sentence_len,))\n",
        "    x_lang = Embedding(num_words, embedding_size)(input_lang)\n",
        "\n",
        "    input_norms = Input((sentence_len,))\n",
        "    x_norms = Embedding(num_words, embedding_size)(input_norms)\n",
        "\n",
        "    x = concatenate([x_pos, x_case])\n",
        "    x = Conv1D(128, 5, padding='same', name='input2_conv1')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = concatenate([x, x_lang])\n",
        "    x = Conv1D(128, 5, padding='same', name='input3_conv1')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = concatenate([x, x_norms])\n",
        "    x = Conv1D(128, 5, padding='same', name='input4_conv1')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = LSTM(128, return_sequences=True)(x)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    # x = BatchNormalization()(x)\n",
        "    x = Dense(100, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # x = Dropout(0.2)(x)\n",
        "    output = Conv1D(num_classes,3, activation=activation_function, padding='same')(x)\n",
        "\n",
        "    model = Model(inputs=[input_pos, input_case, input_lang, input_norms], outputs=output)\n",
        "\n",
        "    model.compile(optimizer='rmsprop', \n",
        "                  loss='categorical_crossentropy')\n",
        "\n",
        "    return model # Возвращаем сформированную модель"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7AQ2g-yljo-"
      },
      "source": [
        "modelL = pseudoLinearSegmentationNet(colors_count, sent_len, num_words, 'softmax', emb_size) # Создаем модель linearSegmentationNet\n",
        "plot_model(modelL, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_9SNZb5iZe1"
      },
      "source": [
        "historyL = modelL.fit(x=[Xtrain_pos, Xtrain_case, Xtrain_lang, Xtrain_norms], y=Ytrain, \n",
        "                      epochs=100, batch_size=64,\n",
        "                      validation_data=([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms], Ytest)) # Обучаем модель на выборке по всем классам\n",
        "modelL.save( '/content/drive/My Drive/tmp/modelL_consistent(production).h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo8RnAYJ5KTf"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyL.history['loss'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyL.history['val_loss'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59nKh6mEiiW1"
      },
      "source": [
        "YpredL_test = modelL.predict([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms])\n",
        "YpredL_test, Ytest_res, paragraphs_test_res = cleansed_data(YpredL_test, Ytest, paragraphs_test_2) # нехорошие данные здесь всё-таки подчищаем"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08hfr42oiiXI"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам\n",
        "print(classification_report(Ytest_res.argmax(axis=-1).flatten(), \n",
        "                            YpredL_test.argmax(axis=-1).flatten(), \n",
        "                            target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjpOsf5lBkl8"
      },
      "source": [
        "**Проверка тестовой выборки (насколько хорошо обучена сеть)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqxDMru2Bkl-"
      },
      "source": [
        "# # выбираем конкретную фразу из тестовой выборки\n",
        "# i = 2 - 1\n",
        "# print(f'{i}.')\n",
        "# print(colorize_the_text(paragraphs_test_res[i], Ytest_res[i], 0, 'pre', sent_len))\n",
        "# print(colorize_the_text(paragraphs_test_res[i], YpredL_test[i], 0, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syDBcKMiBkmA"
      },
      "source": [
        "# # воспроизводим все фразы из тестовой выборки\n",
        "# for i, el in enumerate(paragraphs_test_res):\n",
        "#   print(f'{i+1}.')\n",
        "#   print(colorize_the_text(el, Ytest_res[i], 0, 'pre', sent_len))\n",
        "#   print(colorize_the_text(el, YpredL_test[i], 0, 'pre', sent_len))\n",
        "\n",
        "# # На github не отображается расцветка текста\n",
        "# # Чтобы её посмотреть, воспользуйтесь ссылкой на https://colab.research.google.com/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UlIqDuCsHN5"
      },
      "source": [
        "#### Алгоритм: Embedding + Conv1D , batch_size = 16, activation='softmax'\n",
        "\n",
        "*Параллельная архитектура*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzPMZqITsHN5"
      },
      "source": [
        "'''\n",
        "  Функция создания сети\n",
        "    Входные параметры:\n",
        "    - num_classes - количество классов для рапознавания\n",
        "    - sentence_len - длина предложения\n",
        "    - num_words - количество слов в словаре\n",
        "    - embedding_size - длина эмбединга\n",
        "'''\n",
        "def pseudoLinearSegmentationNet_par(\n",
        "      num_classes = 9,\n",
        "      sentence_len = 200,\n",
        "      num_words = 5000,\n",
        "      activation_function = 'softmax',\n",
        "      embedding_size = 500\n",
        "      ):\n",
        "    input_pos = Input((sentence_len,))\n",
        "    x_pos = Embedding(num_words, embedding_size)(input_pos)\n",
        "    x_pos = Conv1D(64, 3, padding='same', name='input1_conv1')(x_pos)\n",
        "    x_pos = BatchNormalization()(x_pos)\n",
        "    x_pos = Activation('relu')(x_pos)\n",
        "    x_pos = Dropout(0.2)(x_pos)\n",
        "\n",
        "    input_case = Input((sentence_len,))\n",
        "    x_case = Embedding(num_words, embedding_size)(input_case)\n",
        "    x_case = Conv1D(64, 3, padding='same', name='input2_conv1')(x_case)\n",
        "    x_case = BatchNormalization()(x_case)\n",
        "    x_case = Activation('relu')(x_case)\n",
        "    x_case = Dropout(0.2)(x_case)\n",
        "\n",
        "    input_lang = Input((sentence_len,))\n",
        "    x_lang = Embedding(num_words, embedding_size)(input_lang)\n",
        "    x_lang = Conv1D(64, 3, padding='same', name='input3_conv1')(x_lang)\n",
        "    x_lang = BatchNormalization()(x_lang)\n",
        "    x_lang = Activation('relu')(x_lang)\n",
        "    x_lang = Dropout(0.2)(x_lang)\n",
        "\n",
        "    input_norms = Input((sentence_len,))\n",
        "    x_norms = Embedding(num_words, embedding_size)(input_norms)\n",
        "    x_norms = Conv1D(64, 3, padding='same', name='input4_conv1')(x_norms)\n",
        "    x_norms = BatchNormalization()(x_norms)\n",
        "    x_norms = Activation('relu')(x_norms)\n",
        "    x_norms = Dropout(0.2)(x_norms)\n",
        "\n",
        "    x = concatenate([x_pos, x_case, x_lang, x_norms])\n",
        "\n",
        "    x = LSTM(128, return_sequences=True)(x)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    # x = BatchNormalization()(x)\n",
        "    x = Dense(100, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # x = Dropout(0.2)(x)\n",
        "    output = Conv1D(num_classes,3, activation=activation_function, padding='same')(x)\n",
        "\n",
        "    model = Model(inputs=[input_pos, input_case, input_lang, input_norms], outputs=output)\n",
        "\n",
        "    model.compile(optimizer='rmsprop', \n",
        "                  loss='categorical_crossentropy')\n",
        "\n",
        "    return model # Возвращаем сформированную модель"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfVh7JLUsHN6"
      },
      "source": [
        "modelL_par = pseudoLinearSegmentationNet_par(colors_count, sent_len, num_words, 'softmax', emb_size) # Создаем модель linearSegmentationNet\n",
        "plot_model(modelL_par, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZopnDT6ZsHN6"
      },
      "source": [
        "historyL_par = modelL_par.fit(x=[Xtrain_pos, Xtrain_case, Xtrain_lang, Xtrain_norms], y=Ytrain, \n",
        "                      epochs=100, batch_size=64,\n",
        "                      validation_data=([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms], Ytest)) # Обучаем модель на выборке по всем классам\n",
        "\n",
        "modelL_par.save( '/content/drive/My Drive/tmp/modelL_parallel(production).h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_6FBxTh5FgK"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyL_par.history['loss'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyL_par.history['val_loss'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_liJJIEsHN7"
      },
      "source": [
        "YpredL_par = modelL_par.predict([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms])\n",
        "YpredL_par, Ytest_par_res, paragraphs_par_test_res = cleansed_data(YpredL_par, Ytest, paragraphs_test_2) # нехорошие данные здесь всё-таки подчищаем\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSZnSmAsHN7"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам\n",
        "print(classification_report(Ytest_par_res.argmax(axis=-1).flatten(), \n",
        "                            YpredL_par.argmax(axis=-1).flatten(), \n",
        "                            target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSxFeDe1_ohX"
      },
      "source": [
        "**Проверка тестовой выборки (насколько хорошо обучена сеть)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bqiGcWh_ohY"
      },
      "source": [
        "# # выбираем конкретную фразу из тестовой выборки\n",
        "# i = 2 - 1\n",
        "# print(f'{i}.')\n",
        "# print(colorize_the_text(paragraphs_par_test_res[i], Ytest_par_res[i], 0, 'pre', sent_len))\n",
        "# print(colorize_the_text(paragraphs_par_test_res[i], YpredL_par[i], 0, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tfpvAQL_ohb"
      },
      "source": [
        "# # воспроизводим все фразы из тестовой выборки\n",
        "# for i, el in enumerate(paragraphs_par_test_res):\n",
        "#   print(f'{i+1}.')\n",
        "#   print(colorize_the_text(el, Ytest_par_res[i], 0, 'pre', sent_len))\n",
        "#   print(colorize_the_text(el, YpredL_par[i], 0, 'pre', sent_len))\n",
        "\n",
        "# # На github не отображается расцветка текста\n",
        "# # Чтобы её посмотреть, воспользуйтесь ссылкой на https://colab.research.google.com/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWfhv6bHvUOa"
      },
      "source": [
        "### **U-net**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FC9VX4zsnJa"
      },
      "source": [
        "#### U-net classic\n",
        "\n",
        "*Последовательная архитектура*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-n-ivxFLKOF"
      },
      "source": [
        "'''\n",
        "  Функция создания сети\n",
        "    Входные параметры:\n",
        "    - num_classes - количество классов\n",
        "    - sentence_len - длина предложения\n",
        "    - num_words - количество слов в словаре\n",
        "    - embedding_size - длина эмбединга\n",
        "'''\n",
        "\n",
        "def unet(\n",
        "      num_classes = 6,\n",
        "      sentence_len = 200,\n",
        "      num_words = 5000,\n",
        "      activation_function = 'softmax',\n",
        "      embedding_size = 50\n",
        "      ):\n",
        "    input_pos = Input((sentence_len,))\n",
        "    x_pos = Embedding(num_words, embedding_size)(input_pos)\n",
        "    x_pos = Conv1D(64, 3, padding='same', name='input1_conv1')(x_pos)\n",
        "    x_pos = BatchNormalization()(x_pos)\n",
        "    x_pos = Activation('relu')(x_pos)\n",
        "\n",
        "    input_case = Input((sentence_len,))\n",
        "    x_case = Embedding(num_words, embedding_size)(input_case)\n",
        "\n",
        "    input_lang = Input((sentence_len,))\n",
        "    x_lang = Embedding(num_words, embedding_size)(input_lang)\n",
        "\n",
        "    input_norms = Input((sentence_len,))\n",
        "    x_norms = Embedding(num_words, embedding_size)(input_norms)\n",
        "\n",
        "    x = concatenate([x_pos, x_case])\n",
        "    x = Conv1D(128, 5, padding='same', name='input2_conv1')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = concatenate([x, x_lang])\n",
        "    x = Conv1D(128, 5, padding='same', name='input3_conv1')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = concatenate([x, x_norms])\n",
        "    x = Conv1D(128, 5, padding='same', name='input4_conv1')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "\n",
        "    # Block 1\n",
        "    x = Conv1D(64, 3, padding='same', name='block1_conv1')(x) # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same', name='block1_conv2')(x)         # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_1_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_1_out\n",
        "\n",
        "    x = MaxPooling1D()(block_1_out)                                        # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv1D(128, 3, padding='same', name='block2_conv1')(x)        # Добавляем Conv1D-слой с 128-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(128, 3, padding='same', name='block2_conv2')(x)        # Добавляем Conv1D-слой с 128-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_2_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_2_out\n",
        "\n",
        "    x = MaxPooling1D()(block_2_out)                                        # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv1')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv2')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv3')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_3_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_3_out\n",
        "\n",
        "    x = MaxPooling1D()(block_3_out)                                        # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv1')(x)        # Добавляем Conv1D-слой с 512-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv2')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv3')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_4_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_4_out\n",
        "    x = block_4_out \n",
        "\n",
        "    # UP 2\n",
        "    # x = Conv1DTranspose(x, 256, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 256 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(256, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_3_out])                                      # Объединем текущий слой со слоем block_3_out\n",
        "    x = Conv1D(256, 3, padding='same')(x)                             # Добавляем слой Conv1D с 256 нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    # UP 3\n",
        "    # x = Conv1DTranspose(x, 128, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 128 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(128, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_2_out])                                      # Объединем текущий слой со слоем block_2_out\n",
        "    x = Conv1D(128, 3, padding='same')(x)                             # Добавляем слой Conv1D с 128 нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(128, 3, padding='same')(x) # Добавляем слой Conv1D с 128 нейронами\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    # UP 4\n",
        "    # x = Conv1DTranspose(x, 64, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 64 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(64, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_1_out])  # Объединем текущий слой со слоем block_1_out\n",
        "    x = Conv1D(64, 3, padding='same')(x) # Добавляем слой Conv1D с 64 нейронами\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same')(x) # Добавляем слой Conv1D с 64 нейронами\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(num_classes, 3, activation='softmax', padding='same')(x)  # Добавляем Conv1D-Слой с softmax-активацией на num_classes-нейронов\n",
        "\n",
        "    model = Model([input_pos, input_case, input_lang, input_norms], x)                                   # Создаем модель со входами и выходом 'x'\n",
        "\n",
        "    # Компилируем модель \n",
        "    model.compile(optimizer=Adam(),\n",
        "                  loss='categorical_crossentropy')\n",
        "    \n",
        "    return model # Возвращаем сформированную модель"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFlktpNPRtVC"
      },
      "source": [
        "modelUnet = unet(colors_count, sent_len, num_words, 'softmax', emb_size) # Создаем модель unet\n",
        "plot_model(modelUnet, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0-ZaLDp908y"
      },
      "source": [
        "historyUnet = modelUnet.fit(x=[Xtrain_pos, Xtrain_case, Xtrain_lang, Xtrain_norms], y=Ytrain, \n",
        "                            epochs=100, batch_size=64,\n",
        "                            validation_data=([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms], Ytest))                    \n",
        "modelUnet.save( '/content/drive/My Drive/tmp/modelUnet_consistent(production).h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIywdM1ERQIo"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyUnet.history['loss'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyUnet.history['val_loss'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3OL4tZct6hO"
      },
      "source": [
        "YpredUnet = modelUnet.predict([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms])\n",
        "YpredUnet_test, Ytest_res, paragraphs_test_res = cleansed_data(YpredUnet, Ytest, paragraphs_test_2) # нехорошие данные здесь всё-таки подчищаем"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nhGRXh5t6hT"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам\n",
        "print(classification_report(Ytest_res.argmax(axis=-1).flatten(), \n",
        "                            YpredUnet_test.argmax(axis=-1).flatten(), \n",
        "                            target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GFO30AQD6iP"
      },
      "source": [
        "**Проверка тестовой выборки (насколько хорошо обучена сеть)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4uT98nZD6ij"
      },
      "source": [
        "# # выбираем конкретную фразу из тестовой выборки\n",
        "# i = 2 \n",
        "# print(f'{i + 1}.')\n",
        "# print(colorize_the_text(paragraphs_test_res[i], Ytest_res[i], 0, 'pre', sent_len))\n",
        "# print(colorize_the_text(paragraphs_test_res[i], YpredUnet_test[i], 0, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op_68rVZD6is"
      },
      "source": [
        "# # воспроизводим все фразы из тестовой выборки\n",
        "# for i, el in enumerate(paragraphs_test_res):\n",
        "#   print(f'{i+1}.')\n",
        "#   print(colorize_the_text(el, Ytest_res[i], 0, 'pre', sent_len))\n",
        "#   print(colorize_the_text(el, YpredUnet_test[i], 0, 'pre', sent_len))\n",
        "\n",
        "# # На github не отображается расцветка текста\n",
        "# # Чтобы её посмотреть, воспользуйтесь ссылкой на https://colab.research.google.com/ выше"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWY12oi3c65O"
      },
      "source": [
        "#### U-net classic\n",
        "\n",
        "*Параллельная архитектура*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTTEXZ3wc65O"
      },
      "source": [
        "'''\n",
        "  Функция создания сети\n",
        "    Входные параметры:\n",
        "    - num_classes - количество классов\n",
        "    - sentence_len - длина предложения\n",
        "    - num_words - количество слов в словаре\n",
        "    - embedding_size - длина эмбединга\n",
        "'''\n",
        "\n",
        "def unet_par(\n",
        "      num_classes = 6,\n",
        "      sentence_len = 200,\n",
        "      num_words = 5000,\n",
        "      activation_function = 'softmax',\n",
        "      embedding_size = 50\n",
        "      ):\n",
        "    # параллельная архитектура\n",
        "    input_pos = Input((sentence_len,))\n",
        "    x_pos = Embedding(num_words, embedding_size)(input_pos)\n",
        "    x_pos = BatchNormalization()(x_pos)\n",
        "    x_pos = Conv1D(64, 3, padding='same', name='input1_conv1')(x_pos)\n",
        "    x_pos = Activation('relu')(x_pos)\n",
        "    x_pos = Dropout(0.2)(x_pos)\n",
        "\n",
        "    input_case = Input((sentence_len,))\n",
        "    x_case = Embedding(num_words, embedding_size)(input_case)\n",
        "    x_case = BatchNormalization()(x_case)\n",
        "    x_case = Conv1D(64, 3, padding='same', name='input2_conv1')(x_case)\n",
        "    x_case = Activation('relu')(x_case)\n",
        "    x_case = Dropout(0.2)(x_case)\n",
        "\n",
        "    input_lang = Input((sentence_len,))\n",
        "    x_lang = Embedding(num_words, embedding_size)(input_lang)\n",
        "    x_lang = Conv1D(64, 3, padding='same', name='input3_conv1')(x_lang)\n",
        "    x_lang = BatchNormalization()(x_lang)\n",
        "    x_lang = Activation('relu')(x_lang)\n",
        "    x_lang = Dropout(0.2)(x_lang)\n",
        "\n",
        "    input_norms = Input((sentence_len,))\n",
        "    x_norms = Embedding(num_words, embedding_size)(input_norms)\n",
        "    x_norms = Conv1D(64, 3, padding='same', name='input4_conv1')(x_norms)\n",
        "    x_norms = BatchNormalization()(x_norms)\n",
        "    x_norms = Activation('relu')(x_norms)\n",
        "    x_norms = Dropout(0.2)(x_norms)\n",
        "\n",
        "    x = concatenate([x_pos, x_case, x_lang, x_norms])\n",
        "\n",
        "\n",
        "    # Block 1\n",
        "    x = Conv1D(64, 3, padding='same', name='block1_conv1')(x) # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same', name='block1_conv2')(x)         # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_1_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_1_out\n",
        "\n",
        "    x = MaxPooling1D()(block_1_out)                                        # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv1D(128, 3, padding='same', name='block2_conv1')(x)        # Добавляем Conv1D-слой с 128-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(128, 3, padding='same', name='block2_conv2')(x)        # Добавляем Conv1D-слой с 128-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_2_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_2_out\n",
        "\n",
        "    x = MaxPooling1D()(block_2_out)                                        # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv1')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv2')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv3')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_3_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_3_out\n",
        "\n",
        "    x = MaxPooling1D()(block_3_out)                                        # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv1')(x)        # Добавляем Conv1D-слой с 512-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv2')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv3')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_4_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_4_out\n",
        "    x = block_4_out \n",
        "\n",
        "    # UP 2\n",
        "    # x = Conv1DTranspose(x, 256, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 256 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(256, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_3_out])                                      # Объединем текущий слой со слоем block_3_out\n",
        "    x = Conv1D(256, 3, padding='same')(x)                             # Добавляем слой Conv1D с 256 нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    # UP 3\n",
        "    # x = Conv1DTranspose(x, 128, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 128 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(128, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_2_out])                                      # Объединем текущий слой со слоем block_2_out\n",
        "    x = Conv1D(128, 3, padding='same')(x)                             # Добавляем слой Conv1D с 128 нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(128, 3, padding='same')(x) # Добавляем слой Conv1D с 128 нейронами\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    # UP 4\n",
        "    # x = Conv1DTranspose(x, 64, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 64 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(64, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_1_out])  # Объединем текущий слой со слоем block_1_out\n",
        "    x = Conv1D(64, 3, padding='same')(x) # Добавляем слой Conv1D с 64 нейронами\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same')(x) # Добавляем слой Conv1D с 64 нейронами\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(num_classes, 3, activation='softmax', padding='same')(x)  # Добавляем Conv1D-Слой с softmax-активацией на num_classes-нейронов\n",
        "\n",
        "    model = Model([input_pos, input_case, input_lang, input_norms], x)                                   # Создаем модель со входами и выходом 'x'\n",
        "\n",
        "    # Компилируем модель \n",
        "    model.compile(optimizer=Adam(),\n",
        "                  loss='categorical_crossentropy')\n",
        "    \n",
        "    return model # Возвращаем сформированную модель"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOW1Stv3c65O"
      },
      "source": [
        "modelUnet_par = unet_par(colors_count, sent_len, num_words, 'softmax', emb_size) # Создаем модель unet\n",
        "plot_model(modelUnet_par, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpdYFYbnc65O"
      },
      "source": [
        "historyUnet_par = modelUnet_par.fit(x=[Xtrain_pos, Xtrain_case, Xtrain_lang, Xtrain_norms], y=Ytrain, \n",
        "                            epochs=100, batch_size=64,\n",
        "                            validation_data=([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms], Ytest))                    \n",
        "modelUnet.save( '/content/drive/My Drive/tmp/modelUnet_parallel(production).h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j686bf2fc65O"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyUnet_par.history['loss'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyUnet_par.history['val_loss'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzigh6v0c65P"
      },
      "source": [
        "YpredUnet_par = modelUnet_par.predict([Xtest_pos, Xtest_case, Xtest_lang, Xtest_norms])\n",
        "YpredUnet_par, Ytest_res_par, paragraphs_test_res_par = cleansed_data(YpredUnet_par, Ytest, paragraphs_test_2) # нехорошие данные здесь всё-таки подчищаем"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdI5sGojc65P"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам\n",
        "print(classification_report(Ytest_res_par.argmax(axis=-1).flatten(), \n",
        "                            YpredUnet_par.argmax(axis=-1).flatten(), \n",
        "                            target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfTozi3Nc65P"
      },
      "source": [
        "**Проверка тестовой выборки (насколько хорошо обучена сеть)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf3sIyuHc65P"
      },
      "source": [
        "# # выбираем конкретную фразу из тестовой выборки\n",
        "# i = 2\n",
        "# print(f'{i + 1}.')\n",
        "# print(colorize_the_text(paragraphs_test_res_par[i], Ytest_res_par[i], 0, 'pre', sent_len))\n",
        "# print(colorize_the_text(paragraphs_test_res_par[i], YpredUnet_par[i], 0, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGbJw8COc65P"
      },
      "source": [
        "# # воспроизводим все фразы из тестовой выборки\n",
        "# for i, el in enumerate(paragraphs_test_res_par):\n",
        "#   print(f'{i+1}.')\n",
        "#   print(colorize_the_text(el, Ytest_res_par[i], 0, 'pre', sent_len))\n",
        "#   print(colorize_the_text(el, YpredUnet_par[i], 0, 'pre', sent_len))\n",
        "\n",
        "# # На github не отображается расцветка текста\n",
        "# # Чтобы её посмотреть, воспользуйтесь ссылкой на https://colab.research.google.com/ выше"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hydKFjvlfJ_U"
      },
      "source": [
        "# **Checking the neural network model with new data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30kxv4rXcjYm"
      },
      "source": [
        "input_file = '/content/drive/My Drive/Базы/NLP/web.kpi.kharkov.ua.pomada.txt'  # http://web.kpi.kharkov.ua/nanochem/sostav-gubnoj-pomady-infografika/\r\n",
        "# input_file = '/content/drive/My Drive/Базы/NLP/proproprogs.ru.descriptors.txt' # https://proproprogs.ru/python_oop/sozdanie-deskriptorov-klassov\r\n",
        "\r\n",
        "# model_file = '/content/drive/My Drive/tmp/modelEGRU_consistent(production).h5'\r\n",
        "# model_file = '/content/drive/My Drive/tmp/modelEGRU_parallel(production).h5'\r\n",
        "model_file = '/content/drive/My Drive/tmp/modelL_consistent(production).h5'\r\n",
        "# model_file = '/content/drive/My Drive/tmp/modelL_parallel(production).h5'\r\n",
        "# model_file = '/content/drive/My Drive/tmp/modelUnet_consistent(production).h5'\r\n",
        "# model_file = '/content/drive/My Drive/tmp/modelUnet_parallel(production).h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOdTHaTVwmqZ"
      },
      "source": [
        "## Input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qej1FvvAn63N"
      },
      "source": [
        "print('Читаем данные из файла с google-диска')\r\n",
        "f = open(input_file, 'r') #Открываем наш файл для чтения и считываем из него данные \r\n",
        "text = f.read() #Записываем прочитанный текст в переменную \r\n",
        "\r\n",
        "text = re.sub(r'\\n\\n',  r'\\n', text)\r\n",
        "text = re.sub(r'\\n\\n',  r'\\n', text)\r\n",
        "text = re.sub(r'(\\n)([А-Я])',  r'. \\2', text)\r\n",
        "text = re.sub(r'\\n',  r' ', text)\r\n",
        "text = text.replace('..',  '.')\r\n",
        "\r\n",
        "txt_list = sent_tokenize(text, language=\"russian\")\r\n",
        "\r\n",
        "print(f'Длина входного массива (фраз): {len(txt_list)}')\r\n",
        "\r\n",
        "paragraphs_work = []\r\n",
        "max_work_paragraph_len = 0\r\n",
        "for elem in txt_list: \r\n",
        "  lexems = word_tokenize(elem, language=\"russian\")\r\n",
        "  paragraph_length = len(lexems)\r\n",
        "  max_work_paragraph_len = paragraph_length if paragraph_length>max_work_paragraph_len else max_work_paragraph_len\r\n",
        "  if paragraph_length>3 and paragraph_length<129:\r\n",
        "    paragraphs_work.append(lexems) \r\n",
        "  # print(lexems)\r\n",
        "work_paragraphs_amount = len(paragraphs_work)\r\n",
        "print('work_paragraphs_amount',work_paragraphs_amount, 'max_work_paragraph_len',max_work_paragraph_len)\r\n",
        "\r\n",
        "print('Извлекаем морфологические признаки.')\r\n",
        "morph_work = []\r\n",
        "for i, elem in enumerate(paragraphs_work):\r\n",
        "  row = []\r\n",
        "  for j,lexem in enumerate(elem):\r\n",
        "    row.append(morphyus(lexem))\r\n",
        "  morph_work.append(row)\r\n",
        "  print(f'{i+1} done {j+1}!')\r\n",
        "print(f'Длина массива с морфологическими данными: {len(morph_work)}')\r\n",
        "\r\n",
        "# Составим массив нормальных форм слов параллельный массиву paragraphs_work\r\n",
        "# Составим массив частей речи параллельный массиву paragraphs_work\r\n",
        "# Составим массив падежей слов параллельный массиву paragraphs_work\r\n",
        "# Составим массив языковой принадлежности слов параллельный массиву paragraphs_work\r\n",
        "norms_work = []\r\n",
        "pos_work = []\r\n",
        "case_work = []\r\n",
        "lang_work = []\r\n",
        "for i, elem in enumerate(morph_work):\r\n",
        "  norms_sentence = []\r\n",
        "  pos_sentence = []\r\n",
        "  case_sentence = []\r\n",
        "  lang_sentence = []\r\n",
        "  for j,lexem in enumerate(elem):\r\n",
        "    len_descr = len(lexem)\r\n",
        "    lang_sentence.append(lexem[0])\r\n",
        "    norms_sentence.append(lexem[2])\r\n",
        "    # собрать все части речи у лексемы\r\n",
        "    pos = set()\r\n",
        "    for k in range(3, len_descr,13):\r\n",
        "      pos.add(lexem[k])\r\n",
        "    pos_sentence.append(\"\".join(list(pos)))\r\n",
        "    # собрать у лексемы все падежи и \r\n",
        "    # признаки формы глагола, причастия, деепричастия - GRND, PRTS, PRTF, INFN, VERB\r\n",
        "    case = set()\r\n",
        "    if lexem[3] in ['GRND', 'PRTS', 'PRTF', 'INFN', 'VERB']:\r\n",
        "      for k in range(4, len_descr,13):\r\n",
        "        for m in range(k, k+11):\r\n",
        "          case.add(lexem[m] if not lexem[m] in ['None'] else '')\r\n",
        "    else: \r\n",
        "      for k in range(6, len_descr,13):\r\n",
        "        case.add(lexem[k])\r\n",
        "    case_sentence.append(\"\".join(list(case)))\r\n",
        "  norms_work.append(norms_sentence)  \r\n",
        "  pos_work.append(pos_sentence)\r\n",
        "  case_work.append(case_sentence)\r\n",
        "  lang_work.append(lang_sentence)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wRfEL6FxhVk"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKW4f8x_vykl"
      },
      "source": [
        "print('Собираем входные данные в один массив.')\r\n",
        "full_work_pcln = pos_work + case_work + lang_work + norms_work\r\n",
        "full_work_pcln_together = [' '.join(sequence) for sequence in full_work_pcln]\r\n",
        "\r\n",
        "print('Токенизируем входные данные.')\r\n",
        "wX = tokenizer.texts_to_sequences(full_work_pcln_together)\r\n",
        "\r\n",
        "print('Разбиваем wX на входные массивы по типам данных: pos, cases, lang и norms.')\r\n",
        "L = work_paragraphs_amount\r\n",
        "M = 2*L\r\n",
        "N = M+L\r\n",
        "wX_pos = wX[:L]\r\n",
        "wX_case = wX[L:M]\r\n",
        "wX_lang = wX[M:N]\r\n",
        "wX_norms = wX[N:]\r\n",
        "\r\n",
        "print('Выравниваем фразы из токенов до максимального размера абзаца.')\r\n",
        "wX_pos = pad_sequences(wX_pos, sent_len, padding='pre', truncating='post')  # заполнение в начале и обрезка с конца предложения\r\n",
        "wX_case = pad_sequences(wX_case, sent_len, padding='pre', truncating='post')\r\n",
        "wX_lang = pad_sequences(wX_lang, sent_len, padding='pre', truncating='post')\r\n",
        "wX_norms = pad_sequences(wX_norms, sent_len, padding='pre', truncating='post')\r\n",
        "\r\n",
        "print('Сводная информация по подготовленным входным массивам.')\r\n",
        "print('wX_pos.shape=',wX_pos.shape)\r\n",
        "print('wX_case.shape=',wX_case.shape)\r\n",
        "print('wX_lang.shape=',wX_lang.shape)\r\n",
        "print('wX_norms.shape=',wX_norms.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM-m1nTCDF_0"
      },
      "source": [
        "## Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_uFH4lPDTVI"
      },
      "source": [
        "http://web.kpi.kharkov.ua/nanochem/sostav-gubnoj-pomady-infografika/\r\n",
        "\r\n",
        "/content/drive/My Drive/Базы/NLP/web.kpi.kharkov.ua.pomada.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBL7EvCqDKxg"
      },
      "source": [
        "print('Подключаемся к нейросетевой модели.')\r\n",
        "# # параллельная архитектура\r\n",
        "# input_pos = Input((sent_len,))\r\n",
        "# x_pos = Embedding(num_words, emb_size)(input_pos)\r\n",
        "# x_pos = BatchNormalization()(x_pos)\r\n",
        "# x_pos = Bidirectional(GRU(emb_size, return_sequences=True))(x_pos)\r\n",
        "# x_pos = Activation('relu')(x_pos)\r\n",
        "# x_pos = Dropout(0.2)(x_pos)\r\n",
        "\r\n",
        "# input_case = Input((sent_len,))\r\n",
        "# x_case = Embedding(num_words, emb_size)(input_case)\r\n",
        "# x_case = BatchNormalization()(x_case)\r\n",
        "# x_case = Bidirectional(GRU(emb_size, return_sequences=True))(x_case)\r\n",
        "# x_case = Activation('relu')(x_case)\r\n",
        "# x_case = Dropout(0.2)(x_case)\r\n",
        "\r\n",
        "# input_lang = Input((sent_len,))\r\n",
        "# x_lang = Embedding(num_words, emb_size)(input_lang)\r\n",
        "# x_lang = Bidirectional(GRU(emb_size, return_sequences=True))(x_lang)\r\n",
        "# x_lang = BatchNormalization()(x_lang)\r\n",
        "# x_lang = Activation('relu')(x_lang)\r\n",
        "# x_lang = Dropout(0.2)(x_lang)\r\n",
        "\r\n",
        "# input_norms = Input((sent_len,))\r\n",
        "# x_norms = Embedding(num_words, emb_size)(input_norms)\r\n",
        "# x_norms = Bidirectional(GRU(emb_size, return_sequences=True))(x_norms)\r\n",
        "# x_norms = BatchNormalization()(x_norms)\r\n",
        "# x_norms = Activation('relu')(x_norms)\r\n",
        "# x_norms = Dropout(0.2)(x_norms)\r\n",
        "\r\n",
        "# x = concatenate([x_pos, x_case, x_lang, x_norms])\r\n",
        "\r\n",
        "# x = GRU(emb_size, return_sequences=True)(x)\r\n",
        "# x = Dense(100, activation='relu')(x)\r\n",
        "# x = Dropout(0.2)(x)\r\n",
        "\r\n",
        "# output = Dense(colors_count, activation='softmax')(x)\r\n",
        "\r\n",
        "# modelEGRUpar = Model(inputs=[input_pos, input_case, input_lang, input_norms], outputs=output)\r\n",
        "\r\n",
        "# modelEGRUpar.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[dice_coef])\r\n",
        "\r\n",
        "model_cur = load_model(model_file)\r\n",
        "\r\n",
        "print('\\nЗапуск сегментации текста.')\r\n",
        "Ypred_cur = model_cur.predict([wX_pos, wX_case, wX_lang, wX_norms])\r\n",
        "Ypred_cur_res, paragraphs_work_res = cleansed_work_data(Ypred_cur, paragraphs_work) # подчищаем нехорошие данные\r\n",
        "\r\n",
        "print('\\nРезультат сегментации текста.')\r\n",
        "for i, el in enumerate(paragraphs_work_res):\r\n",
        "  print(f'{i+1}.')\r\n",
        "  print(colorize_the_text(el, Ypred_cur_res[i], 0, 'pre', sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54btdVCAiE13"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}