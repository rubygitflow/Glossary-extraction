{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "University of Artificial Intelligence. Glossary extraction. Graduate work. 2020. v.0.6.11.30.min",
      "provenance": [],
      "collapsed_sections": [
        "NguG3E7fNQDC",
        "WUS-Cv30ekv9",
        "kvLTZMldti8v",
        "SpOYDpNQZhvt",
        "4qQTEy0vh8CQ",
        "5YuzBTdbtyFD",
        "zvDohHpAkost",
        "1noKmyeVz8t2",
        "-FC9VX4zsnJa",
        "09KO0jduDIu-",
        "NHDbEEibA6uf"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubygitflow/Glossary-extraction/blob/main/University_of_Artificial_Intelligence_Glossary_extraction_Graduate_work_2020_v_0_6_11_30_min.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3qOPD_U0ag_"
      },
      "source": [
        "# The research project — Glossary extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oNlKeCH0tYI"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NguG3E7fNQDC"
      },
      "source": [
        "# Connecting libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hyWYiPqLdez"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model # Импортируем модели keras: Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, concatenate, Activation, MaxPooling1D, Conv1D, BatchNormalization, Conv2DTranspose, Lambda, LSTM, GRU, Bidirectional # Импортируем стандартные слои keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras import backend as K # Импортируем модуль backend keras'а\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras import utils # Импортируем модуль utils библиотеки tensorflow.keras для получения OHE-представления\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "from tensorflow.keras.preprocessing import image # Импортируем модуль image для работы с изображениями\n",
        "import matplotlib.pyplot as plt # Импортируем модуль pyplot библиотеки matplotlib для построения графиков\n",
        "import collections\n",
        "%matplotlib inline\n",
        "import numpy as np # Импортируем библиотеку numpy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report # статистика точности предсказания по классам\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "\n",
        "import os # Импортируем библиотеку os для работы с фаловой системой\n",
        "from google.colab import files # Импортируем Модуль files для работы с файлами\n",
        "from google.colab import drive # Подключаем гугл-диск"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP4-NkAt96gv"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0tvvy1THBj"
      },
      "source": [
        "# Segmentation of text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUS-Cv30ekv9"
      },
      "source": [
        "## Reading and verifying data (XML parsing). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGmMSJDHWfug"
      },
      "source": [
        "Ссылки на файлы данных:\n",
        "\n",
        "База_словарей_train_6.docx : https://drive.google.com/file/d/13QKXApDf8q8R_BYviX1J80BCJXUOlZUw/view?usp=sharing\n",
        "\n",
        "База_словарей_test_6.docx : https://drive.google.com/file/d/19a1rOu6NuAb-Y_HrHhN7Ssg_ZvxTXHhH/view?usp=sharing\n",
        "\n",
        "База_словарей_train_6.xml : https://drive.google.com/file/d/16lNQFww1QyYRxr8S4OsA2THypqxLQOra/view?usp=sharing\n",
        "\n",
        "База_словарей_test_6.xml : https://drive.google.com/file/d/1-9fG7sdiZnjkarvitPYgX9PFt95eErPi/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CYCV7DDe-5l"
      },
      "source": [
        "color_meaning = {'00ffff':'термин', 'ffff00':'связка', '00ff00':'определение', 'd3d3d3':'расшифровка', \n",
        "                 'ff00ff':'дубль-термин', 'ff0000':'дубль-связка', '808000':'дубль-определение', '0000ff':'дубль-расшифровка',\n",
        "                 'cyan':'термин','yellow':'связка','green':'определение', 'lightgray':'расшифровка',\t                 \n",
        "                 'magenta':'дубль-термин', 'red':'дубль-связка','darkyellow':'дубль-определение', 'blue':'дубль-расшифровка'}\n",
        "\n",
        "                 #создаем словарь соответствий цвет-значение   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWtUZO-tpKl6"
      },
      "source": [
        "color_dic = {'термин':0,'связка':1,'определение':2,'расшифровка':3,\n",
        "\t\t\t       'дубль-термин':4,'дубль-связка':5,'дубль-определение':6,'дубль-расшифровка':7}\n",
        "\n",
        "zero_symbol = 'o'\n",
        "\n",
        "colors_count = len(color_dic) + 1 # количество уникальных тегов (не забываем про тег отсутствия класса для разметки)\n",
        "\n",
        "skip_amount = colors_count  # кол-во пропускаемых строк под легенду при считывании данных (здесь учитывается, что есть пусая строка, в списке тегов последнее значение используется под отсутствующий класс для разметки)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALyGh7kCS7NV"
      },
      "source": [
        "''' \n",
        "  Словарь для цветовой разметки разобранного текста\n",
        "'''\n",
        "class color:\n",
        "  GREY    = '\\33[90m'\n",
        "  RED2    = '\\33[91m'\n",
        "  GREEN2  = '\\33[92m'\n",
        "  YELLOW2 = '\\33[93m'\n",
        "  BLUE2   = '\\33[94m'\n",
        "  VIOLET2 = '\\33[95m'\n",
        "  LI_CYAN = '\\33[96m'\n",
        "  WHITE2  = '\\33[97m'\n",
        "\n",
        "  BLACK  = '\\33[30m'\n",
        "  RED    = '\\33[31m'\n",
        "  GREEN  = '\\33[32m'\n",
        "  YELLOW = '\\33[33m'\n",
        "  BLUE   = '\\33[34m'\n",
        "  VIOLET = '\\33[35m'\n",
        "  CYAN   = '\\33[36m'\n",
        "  WHITE  = '\\33[37m'\n",
        "\n",
        "  GREYBG    = '\\33[100m'\n",
        "  REDBG2    = '\\33[101m'\n",
        "  GREENBG2  = '\\33[102m'\n",
        "  YELLOWBG2 = '\\33[103m'\n",
        "  BLUEBG2   = '\\33[104m'\n",
        "  VIOLETBG2 = '\\33[105m'\n",
        "  LI_CYANBG = '\\33[106m'\n",
        "  WHITEBG2  = '\\33[107m'\n",
        "\n",
        "  BLACKBG  = '\\33[40m'\n",
        "  REDBG    = '\\33[41m'\n",
        "  GREENBG  = '\\33[42m'\n",
        "  YELLOWBG = '\\33[43m'\n",
        "  BLUEBG   = '\\33[44m'\n",
        "  VIOLETBG = '\\33[45m'\n",
        "  CYANBG   = '\\33[46m'\n",
        "  WHITEBG  = '\\33[47m'\n",
        "\n",
        "\n",
        "  BOLD \t\t  = '\\33[1m'\n",
        "  ITALIC \t  = '\\33[3m'\n",
        "  UNDERLINE = '\\33[4m'\n",
        "\n",
        "  BLINK    = '\\33[5m'\n",
        "  BLINK2   = '\\33[6m'\n",
        "  SELECTED = '\\33[7m'\n",
        "  END \t\t = '\\33[0m'\n",
        "  \n",
        "dic_color = {0:color.LI_CYANBG+color.BLACK, 1:color.YELLOWBG2+color.BLACK, 2:color.GREENBG2+color.BLACK, 3:color.WHITEBG+color.BLACK,\n",
        "\t\t\t       4:color.VIOLETBG2+color.WHITE2, 5:color.REDBG2+color.WHITE2, 6:color.YELLOWBG+color.BLACK, 7:color.BLUEBG+color.WHITE2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNXWVxcsGg2Z"
      },
      "source": [
        "colors_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1YJ-aoYe-5q"
      },
      "source": [
        "# названия необходимых тегов для извлекаемых категорий\n",
        "r = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}r'\n",
        "rpr = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}rPr'\n",
        "t = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}t'\n",
        "shd = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}shd'\n",
        "fill = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}fill'\n",
        "highlight = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}highlight'\n",
        "val = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZyCt7bwQvMt"
      },
      "source": [
        "Function library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8vghf1KdWvX"
      },
      "source": [
        "'''\n",
        "  Сервиная функция:\n",
        "  Загрузка выборки заданного количества фраз из файла \n",
        "'''\n",
        "def load_xml(filename, paragraphs_amount):\n",
        "    test_tree = ET.parse(filename)\n",
        "\n",
        "    test_root = test_tree.getroot()\n",
        "    test_body = test_root[0]\n",
        "    test_row_count = paragraphs_amount + skip_amount\n",
        "    return list(iter(test_body))[:test_row_count]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P76fRbVdCZH"
      },
      "source": [
        "'''\n",
        "  Сервиная функция:\n",
        "  Вставка пробела в заданной позиции в строке\n",
        "'''\n",
        "def insert_space(text, index):\n",
        "  return text[:index] + ' ' + text[index:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te8tUbzqXZOv"
      },
      "source": [
        "'''\n",
        "  Основная функция для подготовки обучающей выборки.\n",
        "  Создаём параллельные массивы с текстом и категориями лексем.\n",
        "'''\n",
        "def pars_xml(ps):\n",
        "    # Массивы данных по параграфам\n",
        "    paragraphs = []\n",
        "    tags = []\n",
        "    exception_amount = 0\n",
        "    exception_array = []\n",
        "    # Массив фактических цветов в файле исходных данных\n",
        "    colors = set() # все цвета в документе\n",
        "    # длина массива лексем в параграфе\n",
        "    max_paragraph_len = 0  # Максимальная длина предложения\n",
        "    max_paragraph_len_ind = -1  # Индекс предложения с максимальной длиной\n",
        "\n",
        "    for i, paragraph in enumerate(ps): # проходим по всем абзацам\n",
        "        if i > colors_count - 1:     # пропускаем легенду для цветовой разметки    \n",
        "            phrases_list = paragraph.findall(r) # находим все фразы (часть новости)\n",
        "            p_words = [] # список сущностей в абзаце\n",
        "            tags_w = [] # список семантических цветов для слов в абзаце\n",
        "\n",
        "            last_ind = len(phrases_list)-1\n",
        "            for ind, phrase in enumerate(phrases_list): # проходим по всем фразам в абзаце\n",
        "                words = [] # список сущностей во фразе\n",
        "                y_list = [] # категории для сущностей во фразе\n",
        "\n",
        "                try:\n",
        "                  text = phrase.find(t).text # получаем текст, содержащийся во фразе\n",
        "\n",
        "                  # if i>1300:\n",
        "                  #   print(i,last_ind, text)\n",
        "\n",
        "                  # 1) выделяем пробелом точку (воскл.зн., вопр.зн., двоеточние) в конце предложения\n",
        "                  text_comma = text.strip()\n",
        "                  len_text = len(text_comma)-1\n",
        "                  if ind==last_ind and len_text>0: \n",
        "                    if text_comma[len_text] in ['.', ':', '!', '?']:\n",
        "                      text_comma = insert_space(text_comma, len_text)\n",
        "                    \n",
        "                  # 2) выделяем пробелами символы: , { } ( ) [ ] ; « » „ “ \"\n",
        "                  text_comma = text_comma.replace(',', ' , ')\n",
        "                  text_comma = text_comma.replace('(', ' ( ')\n",
        "                  text_comma = text_comma.replace(')', ' ) ')\n",
        "                  text_comma = text_comma.replace('[', ' [ ')\n",
        "                  text_comma = text_comma.replace(']', ' ] ')\n",
        "                  text_comma = text_comma.replace('{', ' { ')\n",
        "                  text_comma = text_comma.replace('}', ' } ')\n",
        "                  text_comma = text_comma.replace('\"', ' \" ')\n",
        "                  text_comma = text_comma.replace('„', ' „ ')\n",
        "                  text_comma = text_comma.replace('“', ' “ ')\n",
        "                  text_comma = text_comma.replace('«', ' « ')\n",
        "                  text_comma = text_comma.replace('»', ' » ')\n",
        "                  text_comma = text_comma.replace(';', ' ; ')\n",
        "\n",
        "\n",
        "                  # 3) заменяем все виды пробелов одиночным пробелом #32  \n",
        "                  text_comma = text_comma.replace('\\xa0', '')\n",
        "                  text_comma = text_comma.replace('\\x301', '')\n",
        "                  text_comma = text_comma.replace('\\u0301', '')\n",
        "                  text_comma = text_comma.replace('\\ufeff', '')\n",
        "                  text_comma = text_comma.replace('  ', ' ')\n",
        "                  text_comma = text_comma.replace('  ', ' ')\n",
        "                  text_comma = text_comma.strip()      \n",
        "\n",
        "                  # 4) выделяем левым пробелом двоеточие (:) при одновременном выполнении условий: \n",
        "                  # после двух буквенных символов в любом алфавите и перед правым пробелом. \n",
        "                  # https://habr.com/ru/post/349860/\n",
        "                  text_comma = re.sub(r'(\\w\\w)(:)(\\s)',  r'\\1 \\2\\3', text_comma)\n",
        "\n",
        "\n",
        "                  if (len(text_comma)>0):\n",
        "                    text_comma = text_comma.lower()\n",
        "\n",
        "                    style = phrase.find(rpr) # получаем стили фразы\n",
        "                    \n",
        "                    if style.find(shd) is not None: # если размечали через заливку\n",
        "                        color = style.find(shd).attrib[fill] # получаем значение цвета заливки\n",
        "                    elif style.find(highlight) is not None: # если размечали через хайлайт\n",
        "                        color = style.find(highlight).attrib[val] # получаем значение цвета хайлайта\n",
        "                    else:\n",
        "                        color = 'white' # иных вариантов выделения в word нет, значит эта фраза не выделена (белый цвет)\n",
        "                    color = color.lower() # переводим строковое значение цвета в нижний регистр\n",
        "                    meaning = color_meaning[color] if color in color_meaning else '' # если есть цвет в словаре цвет-значение, то получаем значение. в противном случае у фразы значение не было выделено\n",
        "                    \n",
        "                    colors.add(color) # добавляем цвет в словарь всех встреченных цветов, если нужно проанализировать их\n",
        "                    \n",
        "                    words = text_comma.split()\n",
        "                    k = len(words)\n",
        "                    if meaning in color_dic: # если во фразе присутствует какая-то выделяемая сущность\n",
        "                      for j in range(k):\n",
        "                        y_list.append(meaning) # получаем индекс позиции, соответствующей какой-то семантической(смысловой) окраске и устанавливаем по этому индексу абсолютное значение категории\n",
        "                    else:\n",
        "                      for j in range(k):\n",
        "                        y_list.append(zero_symbol)\n",
        "\n",
        "                    p_words += words  # список сущностей в параграфе\n",
        "                    tags_w += y_list # список семантических цветов для слов в параграфе\n",
        "\n",
        "                except Exception:\n",
        "                  exception_amount += 1\n",
        "                  exception_array.append(i)\n",
        "\n",
        "            # print(i, p_words)\n",
        "            if p_words:\n",
        "              paragraphs.append(p_words)\n",
        "              tags.append(tags_w)\n",
        "              \n",
        "            if len(p_words) >= max_paragraph_len: \n",
        "              max_paragraph_len = len(p_words)\n",
        "              max_paragraph_len_ind = i - skip_amount\n",
        "\n",
        "        if i%100==0:\n",
        "            print('line',i)\n",
        "\n",
        "    return     paragraphs, tags, colors, max_paragraph_len, max_paragraph_len_ind, exception_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hie9K_i1Qy3h"
      },
      "source": [
        "'''\n",
        "  Графическая функция:\n",
        "  Подсчёт и вывод на печать статистики о содержании категорий определительных структур в размеченных текстах\n",
        "'''\n",
        "def class_stat(paragraphs, tags, ext_print = False):\n",
        "  classes = np.zeros((colors_count))\n",
        "  len_tags = len(tags)\n",
        "  print('Длина размеченной выборки:', len_tags)\n",
        "  if ext_print:\n",
        "    print(\"\\nПеречень фраз со 100% разметкой:\")\n",
        "  for i, elem in enumerate(tags):\n",
        "    if zero_symbol in elem:\n",
        "      if len(set(elem))==1:\n",
        "        classes[0] += 1\n",
        "    else:\n",
        "      if ext_print:\n",
        "        print(\"paragraphs[{}]\".format(i), '=',paragraphs[i])\n",
        "        print(\"tags[{}]\".format(i), '=',tags[i])\n",
        "        print(\" \")\n",
        "    for item in list(color_dic.keys()):\n",
        "      if item in elem:\n",
        "        real_index = int(color_dic[item]) + 1\n",
        "        classes[real_index] += 1\n",
        "  print(\"\\nСтатистика по классам\")\n",
        "  for i in range(len(classes)):\n",
        "    if i==0:\n",
        "      print(f'Фраз без определительных структур: {classes[0]} - {round(classes[0]*100./len_tags,2)}%')\n",
        "    else:\n",
        "      key=list(color_dic.keys())[list(color_dic.values()).index(i-1)] # ключ по значению\n",
        "      print(f'Фраз с категорией {key}: {classes[i]} - {round(classes[i]*100./len_tags,2)}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuOa4BIgoyRM"
      },
      "source": [
        "'''\n",
        "  Графическая функция:\n",
        "  Построение статистических графиков по входным данным.\n",
        "'''\n",
        "def draw_paragraphs_stat(paragraphs):\n",
        "  # https://pyprog.pro/mpl/mpl_bar.html\n",
        "  counter = []\n",
        "  for elem in paragraphs:\n",
        "    counter.append(len(elem))\n",
        "\n",
        "  plt.figure(figsize=(27, 5))\n",
        "  plt.grid(True)\n",
        "  plt.bar(range(len(paragraphs)), counter)\n",
        "  plt.suptitle('Фактическое распределение длин абзацев (в лексемах)')\n",
        "  plt.show()\n",
        "\n",
        "  dic_counter = collections.Counter(counter)\n",
        "  print('Collections',dic_counter)\n",
        "\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  plt.grid(True)\n",
        "  plt.bar(dic_counter.keys(), dic_counter.values())\n",
        "  plt.suptitle('Статистика кол-ва абзацев по их длинам')\n",
        "  plt.show()\n",
        "\n",
        "  return counter, dic_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvLTZMldti8v"
      },
      "source": [
        "### Training sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7QVhK0ce-6A"
      },
      "source": [
        "paragraphs_amount = 1809\n",
        "ps_train = load_xml('/content/drive/My Drive/Базы/NLP/База_словарей_train_6.xml', paragraphs_amount)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG4RmqkYUJdJ"
      },
      "source": [
        "paragraphs_train, tags_train, train_colors, max_paragraph_len, max_paragraph_len_ind, exception_array  = pars_xml(ps_train)\n",
        "paragraphs_amount = len(paragraphs_train)  # обновляем длину массива на случай наличия в исходной базе пустых строк\n",
        "paragraphs_amount, exception_array # контрольный вывод на размера принятых данных и массива индексов пустых строк"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehD-V4zfe-6H"
      },
      "source": [
        "if len(train_colors) > colors_count:\n",
        "  print('В документе цветов разметки больше допустимого')\n",
        "train_colors # выведем все цвета, которые мы встретили"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtqyJZCuwNi-"
      },
      "source": [
        "print(len(paragraphs_train))\n",
        "print(len(tags_train))\n",
        "\n",
        "print('Максимальная длина предложения — max_paragraph_len =',max_paragraph_len)\n",
        "print('Индекс предложения с максимальной длиной — max_paragraph_len_ind =',max_paragraph_len_ind)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0nnj8jil4ad"
      },
      "source": [
        "# Корректируем максимальную длину предложения для использования U-net сеток\n",
        "max_paragraph_len = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlXDpMje2KGW"
      },
      "source": [
        "train_paragraphs_len = len(paragraphs_train)\n",
        "print('Длина обучающей выборки:', train_paragraphs_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG3LwhSqsRAP"
      },
      "source": [
        "**Извлекаем статистику по абзацам**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yxpLMEpoaJc"
      },
      "source": [
        "train_counter, train_count = draw_paragraphs_stat(paragraphs_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54-6xsxM27b6"
      },
      "source": [
        "**Во скольких фразах встречается тот или  иной класс**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7j0DS0EYJG7"
      },
      "source": [
        "class_stat(paragraphs_train, tags_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpOYDpNQZhvt"
      },
      "source": [
        "### Testing sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNZVFg00cR2T"
      },
      "source": [
        "test_paragraphs_amount = 452\n",
        "ps_test = load_xml('/content/drive/My Drive/Базы/NLP/База_словарей_test_6.xml', test_paragraphs_amount)\n",
        "print(len(ps_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u2KsjhsXqKd"
      },
      "source": [
        "paragraphs_test, tags_test, test_colors, max_test_paragraph_len, max_test_paragraph_len_ind, test_exception_array  = pars_xml(ps_test)\n",
        "test_paragraphs_amount = len(paragraphs_test)  # обновляем длину массива на случай наличия в исходной базе пустых строк\n",
        "test_paragraphs_amount, test_exception_array # контрольный вывод на размера принятых данных и массива индексов пустых строк"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sx9tDiKZhwd"
      },
      "source": [
        "if len(test_colors) > colors_count:\n",
        "  print('В документе цветов разметки больше допустимого')\n",
        "test_colors # выведем все цвета, которые мы встретили"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndFEl2yFZhwh"
      },
      "source": [
        "print(len(paragraphs_test))\n",
        "print(len(tags_test))\n",
        "\n",
        "print('Максимальная длина предложения — max_test_paragraph_len =',max_test_paragraph_len)\n",
        "print('Индекс предложения с максимальной длиной — max_test_paragraph_len_ind =',max_test_paragraph_len_ind)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9uQTUuWZhwk"
      },
      "source": [
        "# Корректируем максимальную длину предложения для использования U-net сеток\n",
        "max_test_paragraph_len = max_paragraph_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsG2lGRlZhwo"
      },
      "source": [
        "**Извлекаем статистику по абзацам**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pk9vJ43rBDJ"
      },
      "source": [
        "test_counter, test_count = draw_paragraphs_stat(paragraphs_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASG-tAwY9X2-"
      },
      "source": [
        "**Во скольких фразах встречается тот или  иной класс**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPdBw9dPP2n6"
      },
      "source": [
        "class_stat(paragraphs_test, tags_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qQTEy0vh8CQ"
      },
      "source": [
        "## Preparing a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0i9mlHIxjN9"
      },
      "source": [
        "Объединим обучающий и тестовый массивы, чтобы воспользоваться общим токенизатором."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5WmvWfoyg-b"
      },
      "source": [
        "all_paragraphs = paragraphs_train + paragraphs_test\n",
        "all_tags = tags_train + tags_test\n",
        "print(len(all_paragraphs))\n",
        "print(len(all_tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkzlZG13zQHr"
      },
      "source": [
        "all_paragraphs_together = [' '.join(sequence) for sequence in all_paragraphs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6oJ_dC-zcl-"
      },
      "source": [
        "all_tags_together = [' '.join(tag) for tag in all_tags]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BiXrOgjzrQg"
      },
      "source": [
        "num_words = 11600\n",
        "sent_len = max_paragraph_len \n",
        "tokenizer = Tokenizer(num_words, filters=' \\t\\n', oov_token='<UNK>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6II2QcmYzuku"
      },
      "source": [
        "tokenizer.fit_on_texts(all_paragraphs_together)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzjVwMSHz2s9"
      },
      "source": [
        "# реальная длина словаря. Сравниваем со значением num_words\n",
        "len(tokenizer.index_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCVBcjOQ06YN"
      },
      "source": [
        "tag_tokenizer = Tokenizer(filters=' ') # colors_count - количество категорий - лучше не указывать, чтобы не потерять категории. Все они автоматически подцедпляются под своими индексами"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Xsex9406YR"
      },
      "source": [
        "tag_tokenizer.fit_on_texts(all_tags_together)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCy4cIEJGX1w"
      },
      "source": [
        "len(tag_tokenizer.index_word) # эта величина равна colors_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPqNICCN4RTv"
      },
      "source": [
        "# prepare color dictionary\n",
        "tag_tokenizer_dic_color = {}\n",
        "tag_tokenizer_dic_color[1] = color.END\n",
        "for i in range(2,10):\n",
        "  tag_tokenizer_dic_color[i] = dic_color[color_dic[tag_tokenizer.index_word[i]]]\n",
        "tag_tokenizer_dic_color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbHUueD3Q7uU"
      },
      "source": [
        "'''\n",
        "  Сервисная функция:\n",
        "  Сделать разметку текста цветом\n",
        "'''\n",
        "def tokens_to_text(x_arr, y_arr, max_len = 0):\n",
        "  res = '' \n",
        "  str_len = 0\n",
        "  def add_col(col, txt):\n",
        "    nonlocal res, str_len\n",
        "    if max_len > 0:\n",
        "      next_len = len(txt)\n",
        "      if str_len + next_len > max_len:\n",
        "        res = res + col + '\\n ' + txt\n",
        "        str_len = next_len + 1\n",
        "      else:\n",
        "        res = res + col + ' ' + txt\n",
        "        str_len += next_len + 1\n",
        "    else:\n",
        "      res = res + col + ' ' + txt\n",
        "\n",
        "  for i, el in enumerate(x_arr):\n",
        "    if el==0:\n",
        "      break\n",
        "    col_ind = y_arr[i].argmax() + 1\n",
        "    add_col(tag_tokenizer_dic_color[col_ind], tokenizer.index_word[el])\n",
        "\n",
        "  return res + color.END"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTaSRv1SVtww"
      },
      "source": [
        "**Проверка входящих и исходящих массивов на соответствие их размерностей** \n",
        "\n",
        "(для выявления факта порчи данных функцией Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE0Vc6Zu06YA"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(all_paragraphs_together)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqwEgXhu06YX"
      },
      "source": [
        "Y = tag_tokenizer.texts_to_sequences(all_tags_together)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm_5HVF2XPi3"
      },
      "source": [
        "len(X)==len(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4C9G1ACWq9O"
      },
      "source": [
        "# Проверка на согласованность входных и выходных данных\n",
        "for i, elem in enumerate(X):\n",
        "  if len(Y[i]) != len(X[i]):\n",
        "    print('X[{}] ='.format(i),X[i])\n",
        "    print('Y[{}] ='.format(i),Y[i])\n",
        "    # Если все массивы равны, то здесь  ничего не должно печататься\n",
        "  else:\n",
        "    pass\n",
        "    # print('len(X[{}]) ='.format(i),len(Y[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlGw1JPNWNcb"
      },
      "source": [
        "**Выравнивание массивов до максимального размера абзаца**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrzsIigR06YH"
      },
      "source": [
        "X = pad_sequences(X, sent_len, padding='post', truncating='post') # заполнение в конце и обрезка с конца предложения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e_5mLwKVrK6"
      },
      "source": [
        "Y = pad_sequences(Y, sent_len, value=1, padding='post', truncating='post') # заполнение в конце и обрезка с конца предложения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA44evFJ06Ye"
      },
      "source": [
        "Y = to_categorical(Y, colors_count+1)  # см. лекцию \"Занятие № 6 Углубленный курс по текстам\" https://youtu.be/Nfw7ZY-WzS8?t=1533"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1rNkXVX06Ym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c5d2c2-7e86-4a45-b822-8fdbd386a7e9"
      },
      "source": [
        "Y = Y[:, :, 1:]  # см. лекцию \"Занятие № 6 Углубленный курс по текстам\" https://youtu.be/Nfw7ZY-WzS8?t=1533"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2261, 128, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmvLbPskCuXD"
      },
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, train_size=train_paragraphs_len, shuffle=False) # Не перемешиваем данные и берём целую часть на тренировочную выборку"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YuzBTdbtyFD"
      },
      "source": [
        "### Total for training and testing samples we have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z8DXUtpDDwX"
      },
      "source": [
        "print('Xtrain.shape=',Xtrain.shape)\n",
        "print('Xtest.shape=',Xtest.shape)\n",
        "print('Ytrain.shape=',Ytrain.shape)\n",
        "print('Ytest.shape=',Ytest.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfpdJqF6oWL8"
      },
      "source": [
        "# Пример окраски размеченного текста\n",
        "print(tokens_to_text(Xtest[30], Ytest[30], 130))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg42wHw3aBXL"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX7UdQfrbjPi"
      },
      "source": [
        "emb_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz9NJZsW6Eua"
      },
      "source": [
        "'''\n",
        "  Собственная функция метрики, обрабатывающая пересечение двух областей\n",
        "'''\n",
        "def dice_coef(y_true, y_pred):\n",
        "    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.) # Возвращаем площадь пересечения деленную на площадь объединения двух областей"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQEsWB9ndoZG"
      },
      "source": [
        "model_dir = \"/content/drive/My Drive/tmp/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MjP0SFXdH7e"
      },
      "source": [
        "'''\n",
        "  callback-функция сохранения на диск лучшей модели в процессе обучения\n",
        "  запись осуществляется на google-диск\n",
        "'''\n",
        "callbacks_list = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
        "        monitor=\"val_dice_coef\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saboeG1-lT-X"
      },
      "source": [
        "### **Linear segmentation neural network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afTSNoMYeMqJ"
      },
      "source": [
        "### **Recurrent neural network GRU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvDohHpAkost"
      },
      "source": [
        "#### Algorithm: Embedding + Bidirectional(GRU) + GRU , batch_size = 128, activation='softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iODBEshzeRKy"
      },
      "source": [
        "input = Input((sent_len,))\n",
        "x = Embedding(num_words, emb_size)(input)\n",
        "x = Bidirectional(GRU(emb_size, return_sequences=True))(x)\n",
        "x = GRU(emb_size * 2, return_sequences=True)(x)\n",
        "output = Dense(colors_count, activation='softmax')(x)\n",
        "\n",
        "modelEGRU = Model(input, output)\n",
        "\n",
        "modelEGRU.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[dice_coef])\n",
        "\n",
        "plot_model(modelEGRU, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE0BmpYCizoz"
      },
      "source": [
        "historyEGRU = modelEGRU.fit(Xtrain, \n",
        "                    Ytrain, \n",
        "                    epochs=100,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(Xtest, Ytest),  \n",
        "                    callbacks=callbacks_list,\n",
        "                    verbose=1)\n",
        "                    \n",
        "modelEGRU.save( '/content/drive/My Drive/tmp/modelEGRU_100epochs(gl_extr).h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wSXmBsHjThu"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyEGRU.history['dice_coef'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyEGRU.history['val_dice_coef'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB-UeyIphMUY"
      },
      "source": [
        "**Проверка тренировочной выборки (насколько хорошо выучены примеры)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArQoQjSBg7S3"
      },
      "source": [
        "YpredEGRU_train = modelEGRU.predict(Xtrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lDDkZ5Cg7S7"
      },
      "source": [
        "for i, el in enumerate(Xtrain):\n",
        "  print(f'{i+1}.')\n",
        "  print(tokens_to_text(el, Ytrain[i]))\n",
        "  print(tokens_to_text(el, YpredEGRU_train[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO92NflnN7uB"
      },
      "source": [
        "**Проверка тестовой выборки (насколько хорошо обучена сеть)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9txMeBMKl6BW"
      },
      "source": [
        "YpredEGRU_test = modelEGRU.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI0-UgJiuZMb"
      },
      "source": [
        "# выбираем конкретную фразу из тестовой выборки\n",
        "i = 2 + 1\n",
        "print(f'{i}.')\n",
        "print(tokens_to_text(Xtest[i], Ytest[i]))\n",
        "print(tokens_to_text(Xtest[i], YpredEGRU_test[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeoeUCb-t5I4"
      },
      "source": [
        "# или воспроизводим все фразы из тестовой выборки\n",
        "for i, el in enumerate(Xtest):\n",
        "  print(f'{i+1}.')\n",
        "  print(tokens_to_text(el, Ytest[i]))\n",
        "  print(tokens_to_text(el, YpredEGRU_test[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK1gMJeZnv3-"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам\n",
        "print(classification_report(Ytest.argmax(axis=-1).flatten(), YpredEGRU_test.argmax(axis=-1).flatten(), target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S1HYdyZHIAA"
      },
      "source": [
        "### **One-dimensional convolution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1noKmyeVz8t2"
      },
      "source": [
        "#### Алгоритм: Embedding + Conv1D , batch_size = 16, activation='softmax'\n",
        "Оборачиваем алгоритм в функцию: **linearSegmentationNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJEhXlIDlTgN"
      },
      "source": [
        "'''\n",
        "  Функция создания сети\n",
        "    Входные параметры:\n",
        "    - num_classes - количество классов\n",
        "    - sentence_len - длина предложения\n",
        "    - num_words - количество слов в словаре\n",
        "    - embedding_size - длина эмбединга\n",
        "'''\n",
        "def linearSegmentationNet(\n",
        "      num_classes = 9,\n",
        "      sentence_len = 200,\n",
        "      num_words = 5000,\n",
        "      activation_function = 'softmax',\n",
        "      embedding_size = 500\n",
        "      ):\n",
        "    input = Input((sentence_len,))                                          # Создаем входной слой с размерностью input_shape\n",
        "    emb = Embedding(num_words, embedding_size)(input)\n",
        "    x = Conv1D(64, 3, padding='same', name='block1_conv1')(emb) # Добавляем Conv1D-слой с 128-нейронами\n",
        "    x = BatchNormalization()(x)                                             # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                               # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(128, 5, padding='same', name='block1_conv2')(x)         # Добавляем Conv1D-слой с 128-нейронами\n",
        "    x = BatchNormalization()(x)                                             # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                               # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(num_classes,3, activation=activation_function, padding='same')(x) # Добавляем Conv1D-Слой с softmax-активацией на num_classes-нейронов\n",
        "\n",
        "    model = Model(input, x)                                             # Создаем модель с входом 'input' и выходом 'x'\n",
        "\n",
        "    # Компилируем модель\n",
        "    # model.compile(optimizer=Adam(lr=1e-3),\n",
        "    model.compile(optimizer='rmsprop', \n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=[dice_coef])\n",
        "    return model # Возвращаем сформированную модель"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7AQ2g-yljo-"
      },
      "source": [
        "modelL = linearSegmentationNet(colors_count, sent_len, num_words, 'softmax', emb_size) # Создаем модель linearSegmentationNet\n",
        "plot_model(modelL, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_9SNZb5iZe1"
      },
      "source": [
        "historyL = modelL.fit(Xtrain, \n",
        "                    Ytrain, \n",
        "                    epochs=100,\n",
        "                    batch_size=16,\n",
        "                    validation_data=(Xtest, Ytest)) # Обучаем модель на выборке по всем классам"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gswffbh8AmEo"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyL.history['dice_coef'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyL.history['val_dice_coef'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59nKh6mEiiW1"
      },
      "source": [
        "YpredL = modelL.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08hfr42oiiXI"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам\n",
        "print(classification_report(Ytest.argmax(axis=-1).flatten(), YpredL.argmax(axis=-1).flatten(), target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FC9VX4zsnJa"
      },
      "source": [
        "### **U-net**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-n-ivxFLKOF"
      },
      "source": [
        "'''\n",
        "  Функция создания сети\n",
        "    Входные параметры:\n",
        "    - num_classes - количество классов\n",
        "    - sentence_len - длина предложения\n",
        "    - num_words - количество слов в словаре\n",
        "    - embedding_size - длина эмбединга\n",
        "'''\n",
        "\n",
        "def unet(\n",
        "      num_classes = 6,\n",
        "      sentence_len = 200,\n",
        "      num_words = 5000,\n",
        "      activation_function = 'softmax',\n",
        "      embedding_size = 50\n",
        "      ):\n",
        "    print('num_classes=',num_classes)\n",
        "    print('sentence_len=',sentence_len)\n",
        "    print('num_words=',num_words)\n",
        "    print('embedding_size=',embedding_size)\n",
        "\n",
        "    txt_input = Input((sentence_len,))                                         # Создаем входной слой с размерностью input_shape\n",
        "    print('txt_input=',txt_input.shape)\n",
        "    emb = Embedding(num_words, embedding_size)(txt_input)\n",
        "    print('emb=',emb.shape)\n",
        "\n",
        "    # Block 1\n",
        "    x = Conv1D(64, 3, padding='same', name='block1_conv1')(emb) # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same', name='block1_conv2')(x)         # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_1_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_1_out\n",
        "\n",
        "    x = MaxPooling1D()(block_1_out)                                        # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv1D(128, 3, padding='same', name='block2_conv1')(x)        # Добавляем Conv1D-слой с 128-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(128, 3, padding='same', name='block2_conv2')(x)        # Добавляем Conv1D-слой с 128-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_2_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_2_out\n",
        "\n",
        "    x = MaxPooling1D()(block_2_out)                                        # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv1')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv2')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv3')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_3_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_3_out\n",
        "\n",
        "    x = MaxPooling1D()(block_3_out)                                        # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv1')(x)        # Добавляем Conv1D-слой с 512-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv2')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv3')(x)        # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_4_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_4_out\n",
        "    x = block_4_out \n",
        "\n",
        "    # UP 2\n",
        "    # x = Conv1DTranspose(x, 256, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 256 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(256, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_3_out])                                      # Объединем текущий слой со слоем block_3_out\n",
        "    x = Conv1D(256, 3, padding='same')(x)                             # Добавляем слой Conv1D с 256 нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    # UP 3\n",
        "    # x = Conv1DTranspose(x, 128, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 128 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(128, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_2_out])                                      # Объединем текущий слой со слоем block_2_out\n",
        "    x = Conv1D(128, 3, padding='same')(x)                             # Добавляем слой Conv1D с 128 нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(128, 3, padding='same')(x) # Добавляем слой Conv1D с 128 нейронами\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    # UP 4\n",
        "    # x = Conv1DTranspose(x, 64, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 64 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(64, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_1_out])  # Объединем текущий слой со слоем block_1_out\n",
        "    x = Conv1D(64, 3, padding='same')(x) # Добавляем слой Conv1D с 64 нейронами\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same')(x) # Добавляем слой Conv1D с 64 нейронами\n",
        "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(num_classes, 3, activation='softmax', padding='same')(x)  # Добавляем Conv1D-Слой с softmax-активацией на num_classes-нейронов\n",
        "\n",
        "    model = Model(txt_input, x) # Создаем модель с входом 'txt_input' и выходом 'x'\n",
        "\n",
        "    # Компилируем модель \n",
        "    model.compile(optimizer=Adam(),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=[dice_coef])\n",
        "    \n",
        "    return model # Возвращаем сформированную модель"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFlktpNPRtVC"
      },
      "source": [
        "modelUnet = unet(colors_count, sent_len, num_words, 'softmax', emb_size) # Создаем модель unet\n",
        "plot_model(modelUnet, show_shapes=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0-ZaLDp908y"
      },
      "source": [
        "historyUnet = modelUnet.fit(Xtrain, \n",
        "                    Ytrain, \n",
        "                    epochs=100,\n",
        "                    batch_size=16,\n",
        "                    validation_data=(Xtest, Ytest),  \n",
        "                    callbacks=callbacks_list,\n",
        "                    verbose=1) \n",
        "                    \n",
        "modelUnet.save( '/content/drive/My Drive/tmp/modelUnet_100epochs(gl_extr).h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIywdM1ERQIo"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyUnet.history['dice_coef'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyUnet.history['val_dice_coef'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3OL4tZct6hO"
      },
      "source": [
        "YpredUnet = modelUnet.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nhGRXh5t6hT"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам\n",
        "print(classification_report(Ytest.argmax(axis=-1).flatten(), YpredUnet.argmax(axis=-1).flatten(), target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GFO30AQD6iP"
      },
      "source": [
        "**Проверка тестовой выборки (насколько хорошо обучена сеть)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4uT98nZD6ij"
      },
      "source": [
        "# выбираем конкретную фразу из тестовой выборки\n",
        "i = 2\n",
        "print(f'{i + 1}.')\n",
        "print(tokens_to_text(Xtest[i], Ytest[i]))\n",
        "print(tokens_to_text(Xtest[i], YpredUnet[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op_68rVZD6is"
      },
      "source": [
        "# воспроизводим все фразы из тестовой выборки\n",
        "for i, el in enumerate(Xtest):\n",
        "  print(f'{i+1}.')\n",
        "  print(tokens_to_text(el, Ytest[i]))\n",
        "  print(tokens_to_text(el, YpredUnet[i]))\n",
        "\n",
        "# На github не отображается расцветка текста\n",
        "# Чтобы её посмотреть, воспользуйтесь ссылкой на https://colab.research.google.com/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09KO0jduDIu-"
      },
      "source": [
        "### **Simplified U-net**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSLJRhASDOxJ"
      },
      "source": [
        "def simpleUnet(\n",
        "      num_classes = 9,\n",
        "      sentence_len = 200,\n",
        "      num_words = 5000,\n",
        "      activation_function = 'softmax',\n",
        "      embedding_size = 50\n",
        "      ):\n",
        "    print('num_classes=',num_classes)\n",
        "    print('sentence_len=',sentence_len)\n",
        "    print('num_words=',num_words)\n",
        "    print('embedding_size=',embedding_size)\n",
        "\n",
        "    txt_input = Input((sentence_len,))                                         # Создаем входной слой с размерностью input_shape\n",
        "    print('txt_input=',txt_input.shape)\n",
        "    emb = Embedding(num_words, embedding_size)(txt_input)\n",
        "    print('emb=',emb.shape)\n",
        "\n",
        "    # Block 1\n",
        "    x = Conv1D(32, 3, padding='same', name='block1_conv1')(emb) # Добавляем Conv1D-слой с 32-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(32, 3, padding='same', name='block1_conv2')(x)         # Добавляем Conv1D-слой с 32-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_1_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_1_out\n",
        "\n",
        "    x = MaxPooling1D()(block_1_out)                                        # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv1D(64, 3, padding='same', name='block2_conv1')(x)         # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same', name='block2_conv2')(x)         # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    block_2_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_2_out\n",
        "\n",
        "    x = MaxPooling1D()(block_2_out)                                        # Добавляем слой MaxPooling1D\n",
        "    \n",
        "    # UP 1\n",
        "    # x = Conv1DTranspose(x, 64, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 64 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(64, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same')(x)                              # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same')(x)                              # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    # UP 2\n",
        "    # x = Conv1DTranspose(x, 32, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 32 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(32, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(32, 3, padding='same')(x)                              # Добавляем Conv1D-слой с 32-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(32, 3, padding='same')(x)                              # Добавляем Conv1D-слой с 32-нейронами\n",
        "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(num_classes,3, activation=activation_function, padding='same')(x) # Добавляем Conv1D-Слой с softmax-активацией на num_classes-нейронов\n",
        "\n",
        "    model = Model(txt_input, x)                                            # Создаем модель с входом 'txt_input' и выходом 'x'\n",
        "\n",
        "    # Компилируем модель\n",
        "    model.compile(optimizer=Adam(lr=1e-3),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=[dice_coef])\n",
        "    \n",
        "    return model                                                           # Возвращаем модель"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w18aR1NJDoW6"
      },
      "source": [
        "modelS = simpleUnet(colors_count, sent_len, num_words, 'softmax', emb_size) # Создаем модель simpleUnet\n",
        "plot_model(modelS, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaKulEo4orfO"
      },
      "source": [
        "historyS = modelS.fit(Xtrain, \n",
        "                    Ytrain, \n",
        "                    epochs=100,\n",
        "                    batch_size=16,\n",
        "                    validation_data=(Xtest, Ytest)) # Обучаем модель на выборке по трем классам"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "butf3qpNPrIs"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyS.history['dice_coef'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyS.history['val_dice_coef'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TupQDnRUuLBB"
      },
      "source": [
        "YpredS = modelS.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96CL1d1auLBV"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам\n",
        "print(classification_report(Ytest.argmax(axis=-1).flatten(), YpredS.argmax(axis=-1).flatten(), target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHDbEEibA6uf"
      },
      "source": [
        "### **Extended U-net**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOjFjIERBFId"
      },
      "source": [
        "def unetWithMask(\n",
        "      num_classes = 9,\n",
        "      sentence_len = 200,\n",
        "      num_words = 5000,\n",
        "      activation_function = 'softmax',\n",
        "      embedding_size = 50\n",
        "      ):\n",
        "    print('num_classes=',num_classes)\n",
        "    print('sentence_len=',sentence_len)\n",
        "    print('num_words=',num_words)\n",
        "    print('embedding_size=',embedding_size)\n",
        "\n",
        "    txt_input = Input((sentence_len,))                                         # Создаем входной слой с размерностью input_shape\n",
        "    print('txt_input=',txt_input.shape)\n",
        "    emb = Embedding(num_words, embedding_size)(txt_input)\n",
        "    print('emb=',emb.shape)\n",
        "\n",
        "    # Block 1\n",
        "    x = Conv1D(64, 3, padding='same', name='block1_conv1')(emb) # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same', name='block1_conv2')(x)      # Добавляем Conv1D-слой с 64-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    block_1_out = Activation('relu')(x)                                 # Добавляем слой Activation и запоминаем в переменной block_1_out\n",
        "    \n",
        "    block_1_out_mask = Conv1D(64, 1, padding='same')(block_1_out)  # Добавляем Conv1D-маску к текущему слою и запоминаем в переменную block_1_out_mask\n",
        "\n",
        "    x = MaxPooling1D()(block_1_out) # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv1D(128, 3, padding='same', name='block2_conv1')(x)     # Добавляем Conv1D-слой с 128-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x) # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(128, 3, padding='same', name='block2_conv2')(x)     # Добавляем Conv1D-слой с 128-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    block_2_out = Activation('relu')(x)                                 # Добавляем слой Activation и запоминаем в переменной block_2_out\n",
        "\n",
        "    block_2_out_mask = Conv1D(128, 1, padding='same')(block_2_out) # Добавляем Conv1D-маску к текущему слою и запоминаем в переменную block_2_out_mask\n",
        "    \n",
        "    x = MaxPooling1D()(block_2_out)                                     # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv1')(x)     # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv2')(x)     # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same', name='block3_conv3')(x)     # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    block_3_out = Activation('relu')(x)                                 # Добавляем слой Activation и запоминаем в переменной block_3_out\n",
        "\n",
        "    block_3_out_mask = Conv1D(256, 1, padding='same')(block_3_out) # Добавляем Conv1D-маску к текущему слою и запоминаем в переменную block_3_out_mask\n",
        "        \n",
        "    x = MaxPooling1D()(block_3_out)                                     # Добавляем слой MaxPooling1D\n",
        "\n",
        "     # Block 4\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv1')(x)     # Добавляем Conv1D-слой с 512-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv2')(x)     # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same', name='block4_conv3')(x)     # Добавляем Conv1D-слой с 256-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    block_4_out = Activation('relu')(x)                                 # Добавляем слой Activation и запоминаем в переменной block_4_out\n",
        "\n",
        "    block_4_out_mask = Conv1D(512, 1, padding='same')(block_4_out) # Добавляем Conv1D-маску к текущему слою и запоминаем в переменную block_4_out_mask\n",
        "            \n",
        "    x = MaxPooling1D()(block_4_out)                                     # Добавляем слой MaxPooling1D\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv1D(512, 3, padding='same', name='block5_conv1')(x)     # Добавляем Conv1D-слой с 512-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same', name='block5_conv2')(x)     # Добавляем Conv1D-слой с 512-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same', name='block5_conv3')(x)     # Добавляем Conv1D-слой с 512-нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "    \n",
        "    for_pretrained_weight = MaxPooling1D()(x)                           # Добавляем слой MaxPooling1D\n",
        " \n",
        "    # UP 1\n",
        "    # x = Conv1DTranspose(x, 512, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 512 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(512, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_4_out, block_4_out_mask])                 # Объединем текущий слой со слоем block_4_out и слоем-маской block_4_out_mask\n",
        "    x = Conv1D(512, 3, padding='same')(x)                          # Добавляем слой Conv1D с 512 нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(512, 3, padding='same')(x)                          # Добавляем слой Conv1D с 512 нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    # UP 2\n",
        "    # x = Conv1DTranspose(x, 256, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 256 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(256, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_3_out, block_3_out_mask])                 # Объединем текущий слой со слоем block_3_out и слоем-маской block_3_out_mask\n",
        "    x = Conv1D(256, 3, padding='same')(x)                          # Добавляем слой Conv1D с 256 нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(256, 3, padding='same')(x)                          # Добавляем слой Conv1D с 256 нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    # UP 3\n",
        "    # x = Conv1DTranspose(x, 128, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 128 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(128, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_2_out, block_2_out_mask])                 # Объединем текущий слой со слоем block_2_out и слоем-маской block_2_out_mask\n",
        "    x = Conv1D(128, 3, padding='same')(x)                          # Добавляем слой Conv1D с 128 нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(128, 3, padding='same')(x)                          # Добавляем слой Conv1D с 128 нейронами\n",
        "    x = BatchNormalization()(x)                                         # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                           # Добавляем слой Activation\n",
        "\n",
        "    # UP 4\n",
        "    # x = Conv1DTranspose(x, 64, 2, strides=2, padding='same')(x)    # Добавляем слой Conv1DTranspose с 64 нейронами\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(x)\n",
        "    x = Conv2DTranspose(64, (2, 1), strides=(2, 1), padding='same')(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    x = BatchNormalization()(x)                                        # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                          # Добавляем слой Activation\n",
        "\n",
        "    x = concatenate([x, block_1_out, block_1_out_mask])                # Объединем текущий слой со слоем block_1_out и слоем-маской block_1_out_mask\n",
        "    x = Conv1D(64, 3, padding='same')(x)                          # Добавляем слой Conv1D с 128 нейронами\n",
        "    x = BatchNormalization()(x)                                        # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                          # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same')(x)                          # Добавляем слой Conv1D с 128 нейронами\n",
        "    x = BatchNormalization()(x)                                        # Добавляем слой BatchNormalization\n",
        "    x = Activation('relu')(x)                                          # Добавляем слой Activation\n",
        "\n",
        "    x = Conv1D(num_classes, 3, activation=activation_function, padding='same')(x) # Добавляем Conv1D-Слой с softmax-активацией на num_classes-нейронов\n",
        "\n",
        "    model = Model(txt_input, x)                                        # Создаем модель с входом 'txt_input' и выходом 'x'\n",
        "\n",
        "    # Компилируем модель \n",
        "    model.compile(optimizer=Adam(),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=[dice_coef])\n",
        "    \n",
        "    return model                                                       # Возвращаем сформированную модель"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqj3pUS7dDiN"
      },
      "source": [
        "modelM3 = unetWithMask(colors_count, sent_len, num_words, 'sigmoid', emb_size) # Создаем модель unetWithMask\n",
        "plot_model(modelM3, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY63xMpXonqn"
      },
      "source": [
        "historyM3 = modelM3.fit(Xtrain, \n",
        "                    Ytrain, \n",
        "                    epochs=100,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(Xtest, Ytest)) #  Обучаем модель на выборке по трем классам на полноразмерных изображениях"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn_bkNnEdDid"
      },
      "source": [
        "# Отобразим график обучения модели\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(historyM3.history['dice_coef'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(historyM3.history['val_dice_coef'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.grid(True)\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECzqpxUtuQw3"
      },
      "source": [
        "YpredM3 = modelM3.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfEAP6MvuQxE"
      },
      "source": [
        "# Встроенная функция sklearn возвращает и точность и полноту по классам\n",
        "print(classification_report(Ytest.argmax(axis=-1).flatten(), YpredM3.argmax(axis=-1).flatten(), target_names=list(tag_tokenizer.index_word.values())))  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}